{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate training/validation/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platform: Windows-10-10.0.19041-SP0 Mozart\n",
      "4.4.0\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "# from multiprocessing import Pool, current_process\n",
    "\n",
    "import random\n",
    "import math \n",
    "from tqdm.autonotebook import tqdm\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from time import time\n",
    "import platform\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from re import sub             \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi']= 120\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "pd.set_option('display.float_format', '{:,.5f}'.format)\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "\n",
    "_platform = platform.platform()\n",
    "print('platform:', _platform, platform.node())\n",
    "if 'Linux' in _platform: # linux\n",
    "    rat_path = '/home/ece/rat_data/'\n",
    "    tsn_path = '/home/ece/tsn_data/'\n",
    "    \n",
    "elif 'macOS' in _platform: # MAC OS X\n",
    "    rat_path = '/Users/cclee/rat_data/'\n",
    "    tsn_path = '/Users/cclee/tsn_data/'     \n",
    "elif 'Windows' in _platform: # Windows\n",
    "    if platform.node()=='Mozart':\n",
    "        rat_path = 'e:/rat_data/'\n",
    "        tsn_path = 'e:/tsn_data/' \n",
    "    else:\n",
    "        rat_path = 'd:/rat_data/'   \n",
    "        tsn_path = 'd:/tsn_data/' \n",
    "  \n",
    "path_rat = pathlib.Path(rat_path)\n",
    "path_tsn = pathlib.Path(tsn_path)\n",
    "path_new_grooming = path_rat.joinpath('new_grooming')\n",
    "\n",
    "print(cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all  [921111, 921112, 921113, 921216, 921217, 921218, 930203, 930204, 930205, 930217, 930218, 930219, 930302, 930303, 930304, 930309, 930310, 930311, 930316, 930317, 930318, 930323, 930324, 930325, 930330]\n",
      "remove_lst  [921112, 921113, 921217, 921218, 930204, 930205, 930218, 930219, 930303, 930304, 930310, 930311, 930317, 930318, 930324, 930325]\n",
      "rat_lst  [921111, 921216, 930203, 930217, 930302, 930309, 930316, 930323, 930330]\n",
      "9 [921111, 921216, 930203, 930217, 930302, 930309, 930316, 930323, 930330]\n"
     ]
    }
   ],
   "source": [
    "finished_lst = [921111, 930302, 930316, 921216, 930203, 930217] \n",
    "\n",
    "def get_rat_lst(allrat=True):\n",
    "    rat_lst = []\n",
    "    if allrat:\n",
    "        all_rat_set = set()\n",
    "        csv_lst = sorted(path_new_grooming.glob('9*.csv'))\n",
    "        for csvf in csv_lst:\n",
    "            tok = csvf.stem.split('_')\n",
    "            rat_date = int(tok[0])\n",
    "            all_rat_set.add(rat_date)\n",
    "        \n",
    "        rat_lst = list(all_rat_set)\n",
    "        rat_lst.sort()\n",
    "        \n",
    "        print('all ', rat_lst)\n",
    "        remove_lst = []\n",
    "        for i in range(len(rat_lst)-1):\n",
    "            if rat_lst[i]+1==rat_lst[i+1]:\n",
    "                remove_lst.append(rat_lst[i+1])\n",
    "          \n",
    "        print('remove_lst ', remove_lst)\n",
    "        for r in remove_lst:\n",
    "            rat_lst.remove(r)    \n",
    "                \n",
    "#         for r in finished_lst:\n",
    "#             rat_lst.remove(r)                        \n",
    "        \n",
    "    print('rat_lst ', rat_lst)\n",
    "    return rat_lst\n",
    "\n",
    "rat_lst = get_rat_lst()\n",
    "print(len(rat_lst), rat_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate train_lst and val_lst files\n",
    "## Rat dependent\n",
    "The user-dependent (UD) training methods require training data from each user, from which a user-specific model is generated. \n",
    "\n",
    "The UI training methods require training data from multiple participants and a generalized model, or a ‘UI’ model, is generated such that it can be applied to unseen users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all  [921111, 921112, 921113, 921216, 921217, 921218, 930203, 930204, 930205, 930217, 930218, 930219, 930302, 930303, 930304, 930309, 930310, 930311, 930316, 930317, 930318, 930323, 930324, 930325, 930330]\n",
      "remove_lst  [921112, 921113, 921217, 921218, 930204, 930205, 930218, 930219, 930303, 930304, 930310, 930311, 930317, 930318, 930324, 930325]\n",
      "rat_lst  [921111, 921216, 930203, 930217, 930302, 930309, 930316, 930323, 930330]\n",
      "d:\\tsn_data\\921111\\frames\n",
      "cannot find path  d:\\tsn_data\\921111\\frames\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a335b4e6e8a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mdf_test_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_lst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test_lst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \"\"\"\n\u001b[1;32m--> 271\u001b[1;33m     op = _Concatenator(\n\u001b[0m\u001b[0;32m    272\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No objects to concatenate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "SPLIT = 1\n",
    "SEED = 43\n",
    "\n",
    "rat_lst = get_rat_lst() # [921111, 930302, 930316, 921216, 930203, 930217]\n",
    "\n",
    "df_train_lst = []\n",
    "df_test_lst = []\n",
    "\n",
    "\n",
    "for rat in rat_lst:\n",
    "    path_frames = path_tsn.joinpath(str(rat), 'frames')\n",
    "    print(path_frames)\n",
    "    if not path_frames.exists():\n",
    "        print('cannot find path ', path_frames)\n",
    "        break    \n",
    "\n",
    "    grooming_train_lst = [str(x) for x in path_frames.iterdir() if x.is_dir() and x.name[0]=='G' and str(rat) in x.name]\n",
    "    non_grooming_train_lst = [str(x) for x in path_frames.iterdir() if x.is_dir() and x.name[0]=='N' and str(rat) in x.name]\n",
    "    \n",
    "    print('grooming_train_lst', len(grooming_train_lst))\n",
    "    print('non_grooming_train_lst', len(non_grooming_train_lst))\n",
    "    x = grooming_train_lst.copy()\n",
    "    \n",
    "    x.extend(non_grooming_train_lst)\n",
    "    print('total x', len(x))\n",
    "    \n",
    "    y = list(np.ones(len(grooming_train_lst), np.int8))\n",
    "    y.extend(list(np.zeros(len(non_grooming_train_lst), np.int8)))\n",
    "    \n",
    "    print('total y',len(y), 'sum y',sum(y), sum(y)/len(y))\n",
    "    \n",
    "    skf = StratifiedShuffleSplit(n_splits=SPLIT, random_state=SEED, test_size=0.2)\n",
    "    for train_index, test_index in skf.split(x, y):\n",
    "        print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index), 'SUM:', len(train_index)+len(test_index))\n",
    "\n",
    "    df = pd.DataFrame({'x':x, 'y':y}) \n",
    "    print(df)\n",
    "    df_train = df.iloc[train_index]\n",
    "    print('df_train', df_train.shape)\n",
    "    \n",
    "    # nongroom 取1.2倍的grooming 數量\n",
    "    num_train_groom = sum(df_train['y'])\n",
    "    num_train_nongroom = int(num_train_groom * 1.2)\n",
    "    \n",
    "    df_train_groom = df_train[df_train['y']==1]\n",
    "    df_train_nongroom1 = df_train[df_train['y']==0]\n",
    "    df_train_nongroom2 = df_train_nongroom1.sample(n=num_train_nongroom, random_state=SEED)\n",
    "    df_train_nongroom3 = df_train_nongroom1.drop(df_train_nongroom2.index)\n",
    "    \n",
    "    df_train = df_train_groom.append(df_train_nongroom2)\n",
    "    df_train = df_train.sample(frac=1, random_state=SEED)\n",
    "    \n",
    "    print('df_train: sum of 1', sum(df_train['y']), sum(df_train['y'])/len(df_train))\n",
    "    print('df_train_nongroom1 (origin)', len(df_train_nongroom1))\n",
    "    print('df_train_nongroom2', len(df_train_nongroom2))\n",
    "    print('df_train_nongroom3 (rest)', len(df_train_nongroom3))\n",
    "    print('df_train', len(df_train))\n",
    "    \n",
    "    df_test = df.iloc[test_index]\n",
    "    print('df_test', df_test.shape)\n",
    "    \n",
    "    # nongroom 取1.2倍的grooming 數量\n",
    "    num_test_groom = sum(df_test['y'])\n",
    "    num_test_nongroom = int(num_test_groom * 1.2)\n",
    "    \n",
    "    df_test_groom = df_test[df_test['y']==1]\n",
    "    df_test_nongroom1 = df_test[df_test['y']==0]\n",
    "    df_test_nongroom2 = df_test_nongroom1.sample(n=num_test_nongroom, random_state=SEED)\n",
    "    df_test_nongroom3 = df_test_nongroom1.drop(df_test_nongroom2.index)\n",
    "    \n",
    "    df_test = df_test_groom.append(df_test_nongroom2)\n",
    "    df_test = df_test.sample(frac=1, random_state=SEED)\n",
    "    \n",
    "    \n",
    "    print('df_test: sum of 1', sum(df_test['y']), sum(df_test['y'])/len(df_test))\n",
    "    print('df_test (after append)', len(df_test))\n",
    "    print('df_test: sum of 1', sum(df_test['y']), sum(df_test['y'])/len(df_test))\n",
    " \n",
    "    df_train_lst.append(df_train)\n",
    "    df_test_lst.append(df_test)\n",
    "    \n",
    "df_train = pd.concat(df_train_lst) \n",
    "df_test = pd.concat(df_test_lst)\n",
    "\n",
    "#### output train file list\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "frame_count_lst = []\n",
    "for row in df_train.itertuples():\n",
    "    path_folder = pathlib.Path(row.x)\n",
    "    img_lst = list(path_folder.glob('img*.jpg'))\n",
    "    frame_count_lst.append(len(img_lst))\n",
    "\n",
    "ss = pd.Series(frame_count_lst)    \n",
    "df_train.insert(1, 'count', ss)\n",
    "print('df_train')\n",
    "display(df_train)\n",
    "\n",
    "path_tsn_data = path_tsn.joinpath('data')\n",
    "if not path_tsn_data.exists():\n",
    "    path_tsn_data.mkdir() \n",
    "fname = path_tsn_data.joinpath('train_lst.txt')\n",
    "df_train.to_csv(fname, header=False, index = False, sep = ' ')\n",
    "\n",
    "###########################################\n",
    "#### output test file list\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "frame_count_lst = []\n",
    "for row in df_test.itertuples():\n",
    "    path_folder = pathlib.Path(row.x)\n",
    "    img_lst = list(path_folder.glob('img*.jpg'))\n",
    "    frame_count_lst.append(len(img_lst))\n",
    "\n",
    "ss = pd.Series(frame_count_lst)    \n",
    "df_test.insert(1, 'count', ss)\n",
    "print('df_test')\n",
    "display(df_test)\n",
    "\n",
    "fname = path_tsn_data.joinpath('val_lst.txt')\n",
    "df_test.to_csv(fname, header=False, index = False, sep = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate test data. You have to specify a rat to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 1\n",
    "SEED = 43\n",
    "\n",
    "rat_lst = [930217] #get_rat_lst() \n",
    "\n",
    "\n",
    "df_test_lst = []\n",
    "\n",
    "\n",
    "for rat in rat_lst:\n",
    "    path_frames = path_tsn.joinpath(str(rat), 'frames')\n",
    "    print(path_frames)\n",
    "    if not path_frames.exists():\n",
    "        print('cannot find path ', path_frames)\n",
    "        break   \n",
    "  \n",
    "\n",
    "    grooming_train_lst = [str(x) for x in path_frames.iterdir() if x.is_dir() and x.name[0]=='G' ]\n",
    "    non_grooming_train_lst = [str(x) for x in path_frames.iterdir() if x.is_dir() and x.name[0]=='N' ]\n",
    "    \n",
    "    print('grooming_train_lst', len(grooming_train_lst))\n",
    "    print('non_grooming_train_lst', len(non_grooming_train_lst))\n",
    "    x = grooming_train_lst.copy()\n",
    "    \n",
    "    x.extend(non_grooming_train_lst)\n",
    "    print('total x', len(x))\n",
    "    \n",
    "    y = list(np.ones(len(grooming_train_lst), np.int8))\n",
    "    y.extend(list(np.zeros(len(non_grooming_train_lst), np.int8)))\n",
    "    \n",
    "    print('total y',len(y), 'sum y',sum(y), sum(y)/len(y))\n",
    "    \n",
    " \n",
    "    df = pd.DataFrame({'x':x, 'y':y}) \n",
    "    print(df)\n",
    " \n",
    "\n",
    "    df_test_lst.append(df)\n",
    "\n",
    "    \n",
    "df_test = pd.concat(df_test_lst)\n",
    "\n",
    "###########################################\n",
    "#### output test file list\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "frame_count_lst = []\n",
    "for row in df_test.itertuples():\n",
    "    path_folder = pathlib.Path(row.x)\n",
    "    img_lst = list(path_folder.glob('img*.jpg'))\n",
    "    frame_count_lst.append(len(img_lst))\n",
    "\n",
    "ss = pd.Series(frame_count_lst)    \n",
    "df_test.insert(1, 'count', ss)\n",
    "print('df_test')\n",
    "print(df_test)\n",
    "\n",
    "path_tsn_data = path_tsn.joinpath('rat')\n",
    "if not path_tsn_data.exists():\n",
    "    path_tsn_data.mkdir() \n",
    "\n",
    "fname = path_tsn_data.joinpath('test_flow_split1.txt')\n",
    "df_test.to_csv(fname, header=False, index = False, sep = ' ')\n",
    "fname = path_tsn_data.joinpath('test_rgb_split1.txt')\n",
    "df_test.to_csv(fname, header=False, index = False, sep = ' ')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
