{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# two stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStream_ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, rgb_channel=3, flow_channel=20, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(TwoStream_ResNet, self).__init__()\n",
    "        # for rgb\n",
    "        self.conv1_a = nn.Conv2d(rgb_channel, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1_a = nn.BatchNorm2d(64)\n",
    "        self.relu_a = nn.ReLU(inplace=True)\n",
    "        self.maxpool_a = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1_a = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2_a = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3_a = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4_a = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        # for optical flow x, y \n",
    "        self.inplanes = 64\n",
    "        self.conv1_b = nn.Conv2d(flow_channel, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1_b = nn.BatchNorm2d(64)\n",
    "        self.relu_b = nn.ReLU(inplace=True)\n",
    "        self.maxpool_b = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1_b = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2_b = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3_b = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4_b = self._make_layer(block, 512, layers[3], stride=2)\n",
    "  \n",
    "        self.bn_f1 = nn.BatchNorm2d(512*2 * block.expansion)\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        # self.fc_aux = nn.Linear(512 * block.expansion, 101)\n",
    "        self.dp = nn.Dropout(p=0.8)\n",
    "        self.fc_action = nn.Linear(512*2 * block.expansion, num_classes)\n",
    "        # self.bn_final = nn.BatchNorm1d(num_classes)\n",
    "        # self.fc2 = nn.Linear(num_classes, num_classes)\n",
    "        # self.fc_final = nn.Linear(num_classes, 101)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_a, x_b):\n",
    "        ## for stream a, rgb, channel 3\n",
    "        x_a = self.conv1_a(x_a)\n",
    "        x_a = self.bn1_a(x_a)\n",
    "        x_a = self.relu_a(x_a)\n",
    "        x_a = self.maxpool_a(x_a)\n",
    "\n",
    "        x_a = self.layer1_a(x_a)\n",
    "        x_a = self.layer2_a(x_a)\n",
    "        x_a = self.layer3_a(x_a)\n",
    "        x_a = self.layer4_a(x_a)\n",
    "\n",
    "        ## for stream b, optical flow, channel 20, 10x, 10y\n",
    "        x_b = self.conv1_b(x_b)\n",
    "        x_b = self.bn1_b(x_b)\n",
    "        x_b = self.relu_b(x_b)\n",
    "        x_b = self.maxpool_b(x_b)\n",
    "\n",
    "        x_b = self.layer1_b(x_b)\n",
    "        x_b = self.layer2_b(x_b)\n",
    "        x_b = self.layer3_b(x_b)\n",
    "        x_b = self.layer4_b(x_b)\n",
    "\n",
    "        ## fusion, concate a, b stream features\n",
    "        x_f= torch.cat([x_a,x_b],dim=1)\n",
    "        x_f = self.bn_f1(x_f)\n",
    "        # x_f = self.ca(x_f) *  x_f  # attention\n",
    "        # x_f = self.sa(x_f) * x_f   # attention\n",
    "\n",
    "\n",
    "        x_f = self.avgpool(x_f)\n",
    "        x_f = x.view(x_f.size(0), -1)\n",
    "        x_f = self.dp(x_f)\n",
    "        x_f = self.fc_action(x_f)\n",
    "        # x = self.bn_final(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.fc_final(x)\n",
    "\n",
    "        return x_f\n",
    "    \n",
    "def gen_two_stream_pretrained(old_params, in_channels):\n",
    "    print('gen_two_stream_pretrained...')\n",
    "    new_params_rgb = collections.OrderedDict()\n",
    "    new_params_flow = collections.OrderedDict()\n",
    "    \n",
    "    # for rgb stream\n",
    "    layer_count = 0\n",
    "    allKeyList = old_params.keys()\n",
    "    allKeyList = list(allKeyList)\n",
    "    for layer_key in allKeyList[:-2]:\n",
    "        lay= layer_key.split('.', 1)\n",
    "        new_layer_name = '_a.'.join(lay)       \n",
    "        new_params_rgb[new_layer_name] = old_params[layer_key]\n",
    "        layer_count += 1\n",
    "            \n",
    "    # for flow stream\n",
    "    layer_count = 0        \n",
    "    for layer_key in allKeyList[:-2]:\n",
    "        lay= layer_key.split('.', 1)\n",
    "        new_layer_name = '_b.'.join(lay)       \n",
    "        if layer_count == 0:\n",
    "            rgb_weight = old_params[layer_key]\n",
    "            # print(type(rgb_weight))\n",
    "            rgb_weight_mean = torch.mean(rgb_weight, dim=1)\n",
    "            # TODO: ugly fix here, why torch.mean() turn tensor to Variable\n",
    "            # print(type(rgb_weight_mean))\n",
    "            flow_weight = rgb_weight_mean.unsqueeze(1).repeat(1,in_channels,1,1)\n",
    "            new_params_flow[new_layer_name] = flow_weight\n",
    "            layer_count += 1\n",
    "            # print(layer_key, new_params[layer_key].size(), type(new_params[layer_key]))\n",
    "        else:\n",
    "            new_params_flow[new_layer_name] = old_params[layer_key]\n",
    "            layer_count += 1\n",
    "            # print(layer_key, new_params[layer_key].size(), type(new_params[layer_key]))\n",
    "    \n",
    "    new_params_rgb.update(new_params_flow)\n",
    "    return new_params_rgb        \n",
    "        \n",
    "        \n",
    "    \n",
    "def twostream_resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = TwoStream_ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_state_dict_from_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def twostream_resnet34(pretrained=False, rgb_channel=3, flow_channel=20, **kwargs):\n",
    "    model = TwoStream_ResNet(BasicBlock, [3, 4, 6, 3], rgb_channel, flow_channel,**kwargs)\n",
    "    if pretrained:\n",
    "        in_channels = flow_channel\n",
    "       \n",
    "        pretrained_dict = load_state_dict_from_url(model_urls['resnet34'])\n",
    "        print(\"model_zoo pretrained_dict's state_dict:\", len(pretrained_dict))\n",
    "        print(type(pretrained_dict))\n",
    "        print('------'*20)\n",
    "        print(\"pretrained_dict's state_dict:\")\n",
    "        for param_tensor in pretrained_dict:\n",
    "            print(param_tensor, \"\\t\", pretrained_dict[param_tensor].size())\n",
    "            \n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        new_pretrained_dict = gen_two_stream_pretrained(pretrained_dict, in_channels)\n",
    "        print('------'*20)\n",
    "        print(\"gen_two_stream_pretrained pretrained_dict's state_dict:\", len(new_pretrained_dict))\n",
    "        for param_tensor in new_pretrained_dict:\n",
    "            print(param_tensor, \"\\t\", new_pretrained_dict[param_tensor].size())\n",
    "\n",
    "        \n",
    "        # 1. filter out unnecessary keys\n",
    "        new_pretrained_dict = {k: v for k, v in new_pretrained_dict.items() if k in model_dict}\n",
    "        print('------'*20)\n",
    "        print(\"filter out unnecessary keys from new_pretrained_dict:\", len(new_pretrained_dict))\n",
    "        for i, param_tensor in enumerate(new_pretrained_dict):\n",
    "            print(i, param_tensor, \"\\t\", new_pretrained_dict[param_tensor].size())\n",
    "            \n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(new_pretrained_dict) \n",
    "        print('------'*20)\n",
    "        print(\"update model_dict's state_dict:\", len(model_dict))\n",
    "        for i, param_tensor in enumerate(model_dict):\n",
    "            print(i, param_tensor, \"\\t\", model_dict[param_tensor].size())\n",
    "\n",
    "        # 3. load the new state dict\n",
    "#         print(model)\n",
    "        model.load_state_dict(model_dict)\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def twostream_resnet50(pretrained=False, rgb_channel=3, flow_channel=20, train_all=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = TwoStream_ResNet(Bottleneck, [3, 4, 6, 3], rgb_channel, flow_channel, **kwargs)\n",
    "    if pretrained:\n",
    "        in_channels = flow_channel\n",
    "       \n",
    "        pretrained_dict = load_state_dict_from_url(model_urls['resnet50'])\n",
    "        # print(\"model_zoo pretrained_dict's state_dict:\", len(pretrained_dict))\n",
    "#         print(type(pretrained_dict))\n",
    "#         print('------'*20)\n",
    "#         print(\"pretrained_dict's state_dict:\")\n",
    "#         for param_tensor in pretrained_dict:\n",
    "#             print(param_tensor, \"\\t\", pretrained_dict[param_tensor].size())\n",
    "            \n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        new_pretrained_dict = gen_two_stream_pretrained(pretrained_dict, in_channels)\n",
    "        # print('------'*20)\n",
    "        # print(\"gen_two_stream_pretrained pretrained_dict's state_dict:\", len(new_pretrained_dict))\n",
    "#         for param_tensor in new_pretrained_dict:\n",
    "#             print(param_tensor, \"\\t\", new_pretrained_dict[param_tensor].size())\n",
    "\n",
    "        \n",
    "        # 1. filter out unnecessary keys\n",
    "        new_pretrained_dict = {k: v for k, v in new_pretrained_dict.items() if k in model_dict}\n",
    "        # print('------'*20)\n",
    "        # print(\"filter out unnecessary keys from new_pretrained_dict:\", len(new_pretrained_dict))\n",
    "#         for i, param_tensor in enumerate(new_pretrained_dict):\n",
    "#             print(i, param_tensor, \"\\t\", new_pretrained_dict[param_tensor].size())\n",
    "            \n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(new_pretrained_dict) \n",
    "        print('------'*20)\n",
    "        print(\"update model_dict's state_dict:\", len(model_dict))\n",
    "        for i, param_tensor in enumerate(model_dict):\n",
    "            print(i, param_tensor, \"\\t\", model_dict[param_tensor].size())\n",
    "\n",
    "        # 3. load the new state dict\n",
    "#         print(model)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        if not train_all:\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'layer4' in name or 'bn1_f' in name or 'fc_action' in name:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False \n",
    "        \n",
    "    return model\n",
    "\n",
    "def twostream_resnet50_aux(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = TwoStream_ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        # model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "        pretrained_dict = load_state_dict_from_url(model_urls['resnet50'])\n",
    "\n",
    "        model_dict = model.state_dict()\n",
    "        fc_origin_weight = pretrained_dict[\"fc.weight\"].data.numpy()\n",
    "        fc_origin_bias = pretrained_dict[\"fc.bias\"].data.numpy()\n",
    "\n",
    "        # 1. filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict) \n",
    "        # print(model_dict)\n",
    "        fc_new_weight = model_dict[\"fc_aux.weight\"].numpy() \n",
    "        fc_new_bias = model_dict[\"fc_aux.bias\"].numpy() \n",
    "\n",
    "        fc_new_weight[:1000, :] = fc_origin_weight\n",
    "        fc_new_bias[:1000] = fc_origin_bias\n",
    "\n",
    "        model_dict[\"fc_aux.weight\"] = torch.from_numpy(fc_new_weight)\n",
    "        model_dict[\"fc_aux.bias\"] = torch.from_numpy(fc_new_bias)\n",
    "\n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "def twostream_resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = TwoStream_ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_state_dict_from_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def twostream_resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = TwoStream_ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        # model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "        pretrained_dict = load_state_dict_from_url(model_urls['resnet152'])\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        # 1. filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict) \n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_zoo pretrained_dict's state_dict: 182\n",
      "<class 'collections.OrderedDict'>\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "pretrained_dict's state_dict:\n",
      "conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "bn1.running_mean \t torch.Size([64])\n",
      "bn1.running_var \t torch.Size([64])\n",
      "bn1.weight \t torch.Size([64])\n",
      "bn1.bias \t torch.Size([64])\n",
      "layer1.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "layer1.0.bn1.running_var \t torch.Size([64])\n",
      "layer1.0.bn1.weight \t torch.Size([64])\n",
      "layer1.0.bn1.bias \t torch.Size([64])\n",
      "layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "layer1.0.bn2.running_var \t torch.Size([64])\n",
      "layer1.0.bn2.weight \t torch.Size([64])\n",
      "layer1.0.bn2.bias \t torch.Size([64])\n",
      "layer1.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "layer1.1.bn1.running_var \t torch.Size([64])\n",
      "layer1.1.bn1.weight \t torch.Size([64])\n",
      "layer1.1.bn1.bias \t torch.Size([64])\n",
      "layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "layer1.1.bn2.running_var \t torch.Size([64])\n",
      "layer1.1.bn2.weight \t torch.Size([64])\n",
      "layer1.1.bn2.bias \t torch.Size([64])\n",
      "layer1.2.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.2.bn1.running_mean \t torch.Size([64])\n",
      "layer1.2.bn1.running_var \t torch.Size([64])\n",
      "layer1.2.bn1.weight \t torch.Size([64])\n",
      "layer1.2.bn1.bias \t torch.Size([64])\n",
      "layer1.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1.2.bn2.running_mean \t torch.Size([64])\n",
      "layer1.2.bn2.running_var \t torch.Size([64])\n",
      "layer1.2.bn2.weight \t torch.Size([64])\n",
      "layer1.2.bn2.bias \t torch.Size([64])\n",
      "layer2.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "layer2.0.bn1.running_var \t torch.Size([128])\n",
      "layer2.0.bn1.weight \t torch.Size([128])\n",
      "layer2.0.bn1.bias \t torch.Size([128])\n",
      "layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "layer2.0.bn2.running_var \t torch.Size([128])\n",
      "layer2.0.bn2.weight \t torch.Size([128])\n",
      "layer2.0.bn2.bias \t torch.Size([128])\n",
      "layer2.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "layer2.0.downsample.1.running_mean \t torch.Size([128])\n",
      "layer2.0.downsample.1.running_var \t torch.Size([128])\n",
      "layer2.0.downsample.1.weight \t torch.Size([128])\n",
      "layer2.0.downsample.1.bias \t torch.Size([128])\n",
      "layer2.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "layer2.1.bn1.running_var \t torch.Size([128])\n",
      "layer2.1.bn1.weight \t torch.Size([128])\n",
      "layer2.1.bn1.bias \t torch.Size([128])\n",
      "layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "layer2.1.bn2.running_var \t torch.Size([128])\n",
      "layer2.1.bn2.weight \t torch.Size([128])\n",
      "layer2.1.bn2.bias \t torch.Size([128])\n",
      "layer2.2.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.2.bn1.running_mean \t torch.Size([128])\n",
      "layer2.2.bn1.running_var \t torch.Size([128])\n",
      "layer2.2.bn1.weight \t torch.Size([128])\n",
      "layer2.2.bn1.bias \t torch.Size([128])\n",
      "layer2.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.2.bn2.running_mean \t torch.Size([128])\n",
      "layer2.2.bn2.running_var \t torch.Size([128])\n",
      "layer2.2.bn2.weight \t torch.Size([128])\n",
      "layer2.2.bn2.bias \t torch.Size([128])\n",
      "layer2.3.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.3.bn1.running_mean \t torch.Size([128])\n",
      "layer2.3.bn1.running_var \t torch.Size([128])\n",
      "layer2.3.bn1.weight \t torch.Size([128])\n",
      "layer2.3.bn1.bias \t torch.Size([128])\n",
      "layer2.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2.3.bn2.running_mean \t torch.Size([128])\n",
      "layer2.3.bn2.running_var \t torch.Size([128])\n",
      "layer2.3.bn2.weight \t torch.Size([128])\n",
      "layer2.3.bn2.bias \t torch.Size([128])\n",
      "layer3.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "layer3.0.bn1.running_var \t torch.Size([256])\n",
      "layer3.0.bn1.weight \t torch.Size([256])\n",
      "layer3.0.bn1.bias \t torch.Size([256])\n",
      "layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "layer3.0.bn2.running_var \t torch.Size([256])\n",
      "layer3.0.bn2.weight \t torch.Size([256])\n",
      "layer3.0.bn2.bias \t torch.Size([256])\n",
      "layer3.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "layer3.0.downsample.1.running_mean \t torch.Size([256])\n",
      "layer3.0.downsample.1.running_var \t torch.Size([256])\n",
      "layer3.0.downsample.1.weight \t torch.Size([256])\n",
      "layer3.0.downsample.1.bias \t torch.Size([256])\n",
      "layer3.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "layer3.1.bn1.running_var \t torch.Size([256])\n",
      "layer3.1.bn1.weight \t torch.Size([256])\n",
      "layer3.1.bn1.bias \t torch.Size([256])\n",
      "layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "layer3.1.bn2.running_var \t torch.Size([256])\n",
      "layer3.1.bn2.weight \t torch.Size([256])\n",
      "layer3.1.bn2.bias \t torch.Size([256])\n",
      "layer3.2.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.2.bn1.running_mean \t torch.Size([256])\n",
      "layer3.2.bn1.running_var \t torch.Size([256])\n",
      "layer3.2.bn1.weight \t torch.Size([256])\n",
      "layer3.2.bn1.bias \t torch.Size([256])\n",
      "layer3.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.2.bn2.running_mean \t torch.Size([256])\n",
      "layer3.2.bn2.running_var \t torch.Size([256])\n",
      "layer3.2.bn2.weight \t torch.Size([256])\n",
      "layer3.2.bn2.bias \t torch.Size([256])\n",
      "layer3.3.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.3.bn1.running_mean \t torch.Size([256])\n",
      "layer3.3.bn1.running_var \t torch.Size([256])\n",
      "layer3.3.bn1.weight \t torch.Size([256])\n",
      "layer3.3.bn1.bias \t torch.Size([256])\n",
      "layer3.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.3.bn2.running_mean \t torch.Size([256])\n",
      "layer3.3.bn2.running_var \t torch.Size([256])\n",
      "layer3.3.bn2.weight \t torch.Size([256])\n",
      "layer3.3.bn2.bias \t torch.Size([256])\n",
      "layer3.4.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.4.bn1.running_mean \t torch.Size([256])\n",
      "layer3.4.bn1.running_var \t torch.Size([256])\n",
      "layer3.4.bn1.weight \t torch.Size([256])\n",
      "layer3.4.bn1.bias \t torch.Size([256])\n",
      "layer3.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.4.bn2.running_mean \t torch.Size([256])\n",
      "layer3.4.bn2.running_var \t torch.Size([256])\n",
      "layer3.4.bn2.weight \t torch.Size([256])\n",
      "layer3.4.bn2.bias \t torch.Size([256])\n",
      "layer3.5.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.5.bn1.running_mean \t torch.Size([256])\n",
      "layer3.5.bn1.running_var \t torch.Size([256])\n",
      "layer3.5.bn1.weight \t torch.Size([256])\n",
      "layer3.5.bn1.bias \t torch.Size([256])\n",
      "layer3.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3.5.bn2.running_mean \t torch.Size([256])\n",
      "layer3.5.bn2.running_var \t torch.Size([256])\n",
      "layer3.5.bn2.weight \t torch.Size([256])\n",
      "layer3.5.bn2.bias \t torch.Size([256])\n",
      "layer4.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "layer4.0.bn1.running_var \t torch.Size([512])\n",
      "layer4.0.bn1.weight \t torch.Size([512])\n",
      "layer4.0.bn1.bias \t torch.Size([512])\n",
      "layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "layer4.0.bn2.running_var \t torch.Size([512])\n",
      "layer4.0.bn2.weight \t torch.Size([512])\n",
      "layer4.0.bn2.bias \t torch.Size([512])\n",
      "layer4.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.running_mean \t torch.Size([512])\n",
      "layer4.0.downsample.1.running_var \t torch.Size([512])\n",
      "layer4.0.downsample.1.weight \t torch.Size([512])\n",
      "layer4.0.downsample.1.bias \t torch.Size([512])\n",
      "layer4.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "layer4.1.bn1.running_var \t torch.Size([512])\n",
      "layer4.1.bn1.weight \t torch.Size([512])\n",
      "layer4.1.bn1.bias \t torch.Size([512])\n",
      "layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "layer4.1.bn2.running_var \t torch.Size([512])\n",
      "layer4.1.bn2.weight \t torch.Size([512])\n",
      "layer4.1.bn2.bias \t torch.Size([512])\n",
      "layer4.2.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.2.bn1.running_mean \t torch.Size([512])\n",
      "layer4.2.bn1.running_var \t torch.Size([512])\n",
      "layer4.2.bn1.weight \t torch.Size([512])\n",
      "layer4.2.bn1.bias \t torch.Size([512])\n",
      "layer4.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4.2.bn2.running_mean \t torch.Size([512])\n",
      "layer4.2.bn2.running_var \t torch.Size([512])\n",
      "layer4.2.bn2.weight \t torch.Size([512])\n",
      "layer4.2.bn2.bias \t torch.Size([512])\n",
      "fc.weight \t torch.Size([1000, 512])\n",
      "fc.bias \t torch.Size([1000])\n",
      "gen_two_stream_pretrained...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "gen_two_stream_pretrained pretrained_dict's state_dict: 360\n",
      "conv1_a.weight \t torch.Size([64, 3, 7, 7])\n",
      "bn1_a.running_mean \t torch.Size([64])\n",
      "bn1_a.running_var \t torch.Size([64])\n",
      "bn1_a.weight \t torch.Size([64])\n",
      "bn1_a.bias \t torch.Size([64])\n",
      "layer1_a.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1_a.0.bn1.running_mean \t torch.Size([64])\n",
      "layer1_a.0.bn1.running_var \t torch.Size([64])\n",
      "layer1_a.0.bn1.weight \t torch.Size([64])\n",
      "layer1_a.0.bn1.bias \t torch.Size([64])\n",
      "layer1_a.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1_a.0.bn2.running_mean \t torch.Size([64])\n",
      "layer1_a.0.bn2.running_var \t torch.Size([64])\n",
      "layer1_a.0.bn2.weight \t torch.Size([64])\n",
      "layer1_a.0.bn2.bias \t torch.Size([64])\n",
      "layer1_a.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1_a.1.bn1.running_mean \t torch.Size([64])\n",
      "layer1_a.1.bn1.running_var \t torch.Size([64])\n",
      "layer1_a.1.bn1.weight \t torch.Size([64])\n",
      "layer1_a.1.bn1.bias \t torch.Size([64])\n",
      "layer1_a.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1_a.1.bn2.running_mean \t torch.Size([64])\n",
      "layer1_a.1.bn2.running_var \t torch.Size([64])\n",
      "layer1_a.1.bn2.weight \t torch.Size([64])\n",
      "layer1_a.1.bn2.bias \t torch.Size([64])\n",
      "layer1_a.2.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1_a.2.bn1.running_mean \t torch.Size([64])\n",
      "layer1_a.2.bn1.running_var \t torch.Size([64])\n",
      "layer1_a.2.bn1.weight \t torch.Size([64])\n",
      "layer1_a.2.bn1.bias \t torch.Size([64])\n",
      "layer1_a.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1_a.2.bn2.running_mean \t torch.Size([64])\n",
      "layer1_a.2.bn2.running_var \t torch.Size([64])\n",
      "layer1_a.2.bn2.weight \t torch.Size([64])\n",
      "layer1_a.2.bn2.bias \t torch.Size([64])\n",
      "layer2_a.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "layer2_a.0.bn1.running_mean \t torch.Size([128])\n",
      "layer2_a.0.bn1.running_var \t torch.Size([128])\n",
      "layer2_a.0.bn1.weight \t torch.Size([128])\n",
      "layer2_a.0.bn1.bias \t torch.Size([128])\n",
      "layer2_a.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_a.0.bn2.running_mean \t torch.Size([128])\n",
      "layer2_a.0.bn2.running_var \t torch.Size([128])\n",
      "layer2_a.0.bn2.weight \t torch.Size([128])\n",
      "layer2_a.0.bn2.bias \t torch.Size([128])\n",
      "layer2_a.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "layer2_a.0.downsample.1.running_mean \t torch.Size([128])\n",
      "layer2_a.0.downsample.1.running_var \t torch.Size([128])\n",
      "layer2_a.0.downsample.1.weight \t torch.Size([128])\n",
      "layer2_a.0.downsample.1.bias \t torch.Size([128])\n",
      "layer2_a.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_a.1.bn1.running_mean \t torch.Size([128])\n",
      "layer2_a.1.bn1.running_var \t torch.Size([128])\n",
      "layer2_a.1.bn1.weight \t torch.Size([128])\n",
      "layer2_a.1.bn1.bias \t torch.Size([128])\n",
      "layer2_a.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_a.1.bn2.running_mean \t torch.Size([128])\n",
      "layer2_a.1.bn2.running_var \t torch.Size([128])\n",
      "layer2_a.1.bn2.weight \t torch.Size([128])\n",
      "layer2_a.1.bn2.bias \t torch.Size([128])\n",
      "layer2_a.2.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_a.2.bn1.running_mean \t torch.Size([128])\n",
      "layer2_a.2.bn1.running_var \t torch.Size([128])\n",
      "layer2_a.2.bn1.weight \t torch.Size([128])\n",
      "layer2_a.2.bn1.bias \t torch.Size([128])\n",
      "layer2_a.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_a.2.bn2.running_mean \t torch.Size([128])\n",
      "layer2_a.2.bn2.running_var \t torch.Size([128])\n",
      "layer2_a.2.bn2.weight \t torch.Size([128])\n",
      "layer2_a.2.bn2.bias \t torch.Size([128])\n",
      "layer2_a.3.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_a.3.bn1.running_mean \t torch.Size([128])\n",
      "layer2_a.3.bn1.running_var \t torch.Size([128])\n",
      "layer2_a.3.bn1.weight \t torch.Size([128])\n",
      "layer2_a.3.bn1.bias \t torch.Size([128])\n",
      "layer2_a.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_a.3.bn2.running_mean \t torch.Size([128])\n",
      "layer2_a.3.bn2.running_var \t torch.Size([128])\n",
      "layer2_a.3.bn2.weight \t torch.Size([128])\n",
      "layer2_a.3.bn2.bias \t torch.Size([128])\n",
      "layer3_a.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "layer3_a.0.bn1.running_mean \t torch.Size([256])\n",
      "layer3_a.0.bn1.running_var \t torch.Size([256])\n",
      "layer3_a.0.bn1.weight \t torch.Size([256])\n",
      "layer3_a.0.bn1.bias \t torch.Size([256])\n",
      "layer3_a.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_a.0.bn2.running_mean \t torch.Size([256])\n",
      "layer3_a.0.bn2.running_var \t torch.Size([256])\n",
      "layer3_a.0.bn2.weight \t torch.Size([256])\n",
      "layer3_a.0.bn2.bias \t torch.Size([256])\n",
      "layer3_a.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "layer3_a.0.downsample.1.running_mean \t torch.Size([256])\n",
      "layer3_a.0.downsample.1.running_var \t torch.Size([256])\n",
      "layer3_a.0.downsample.1.weight \t torch.Size([256])\n",
      "layer3_a.0.downsample.1.bias \t torch.Size([256])\n",
      "layer3_a.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_a.1.bn1.running_mean \t torch.Size([256])\n",
      "layer3_a.1.bn1.running_var \t torch.Size([256])\n",
      "layer3_a.1.bn1.weight \t torch.Size([256])\n",
      "layer3_a.1.bn1.bias \t torch.Size([256])\n",
      "layer3_a.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_a.1.bn2.running_mean \t torch.Size([256])\n",
      "layer3_a.1.bn2.running_var \t torch.Size([256])\n",
      "layer3_a.1.bn2.weight \t torch.Size([256])\n",
      "layer3_a.1.bn2.bias \t torch.Size([256])\n",
      "layer3_a.2.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_a.2.bn1.running_mean \t torch.Size([256])\n",
      "layer3_a.2.bn1.running_var \t torch.Size([256])\n",
      "layer3_a.2.bn1.weight \t torch.Size([256])\n",
      "layer3_a.2.bn1.bias \t torch.Size([256])\n",
      "layer3_a.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_a.2.bn2.running_mean \t torch.Size([256])\n",
      "layer3_a.2.bn2.running_var \t torch.Size([256])\n",
      "layer3_a.2.bn2.weight \t torch.Size([256])\n",
      "layer3_a.2.bn2.bias \t torch.Size([256])\n",
      "layer3_a.3.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_a.3.bn1.running_mean \t torch.Size([256])\n",
      "layer3_a.3.bn1.running_var \t torch.Size([256])\n",
      "layer3_a.3.bn1.weight \t torch.Size([256])\n",
      "layer3_a.3.bn1.bias \t torch.Size([256])\n",
      "layer3_a.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_a.3.bn2.running_mean \t torch.Size([256])\n",
      "layer3_a.3.bn2.running_var \t torch.Size([256])\n",
      "layer3_a.3.bn2.weight \t torch.Size([256])\n",
      "layer3_a.3.bn2.bias \t torch.Size([256])\n",
      "layer3_a.4.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_a.4.bn1.running_mean \t torch.Size([256])\n",
      "layer3_a.4.bn1.running_var \t torch.Size([256])\n",
      "layer3_a.4.bn1.weight \t torch.Size([256])\n",
      "layer3_a.4.bn1.bias \t torch.Size([256])\n",
      "layer3_a.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_a.4.bn2.running_mean \t torch.Size([256])\n",
      "layer3_a.4.bn2.running_var \t torch.Size([256])\n",
      "layer3_a.4.bn2.weight \t torch.Size([256])\n",
      "layer3_a.4.bn2.bias \t torch.Size([256])\n",
      "layer3_a.5.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_a.5.bn1.running_mean \t torch.Size([256])\n",
      "layer3_a.5.bn1.running_var \t torch.Size([256])\n",
      "layer3_a.5.bn1.weight \t torch.Size([256])\n",
      "layer3_a.5.bn1.bias \t torch.Size([256])\n",
      "layer3_a.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_a.5.bn2.running_mean \t torch.Size([256])\n",
      "layer3_a.5.bn2.running_var \t torch.Size([256])\n",
      "layer3_a.5.bn2.weight \t torch.Size([256])\n",
      "layer3_a.5.bn2.bias \t torch.Size([256])\n",
      "layer4_a.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "layer4_a.0.bn1.running_mean \t torch.Size([512])\n",
      "layer4_a.0.bn1.running_var \t torch.Size([512])\n",
      "layer4_a.0.bn1.weight \t torch.Size([512])\n",
      "layer4_a.0.bn1.bias \t torch.Size([512])\n",
      "layer4_a.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4_a.0.bn2.running_mean \t torch.Size([512])\n",
      "layer4_a.0.bn2.running_var \t torch.Size([512])\n",
      "layer4_a.0.bn2.weight \t torch.Size([512])\n",
      "layer4_a.0.bn2.bias \t torch.Size([512])\n",
      "layer4_a.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "layer4_a.0.downsample.1.running_mean \t torch.Size([512])\n",
      "layer4_a.0.downsample.1.running_var \t torch.Size([512])\n",
      "layer4_a.0.downsample.1.weight \t torch.Size([512])\n",
      "layer4_a.0.downsample.1.bias \t torch.Size([512])\n",
      "layer4_a.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4_a.1.bn1.running_mean \t torch.Size([512])\n",
      "layer4_a.1.bn1.running_var \t torch.Size([512])\n",
      "layer4_a.1.bn1.weight \t torch.Size([512])\n",
      "layer4_a.1.bn1.bias \t torch.Size([512])\n",
      "layer4_a.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4_a.1.bn2.running_mean \t torch.Size([512])\n",
      "layer4_a.1.bn2.running_var \t torch.Size([512])\n",
      "layer4_a.1.bn2.weight \t torch.Size([512])\n",
      "layer4_a.1.bn2.bias \t torch.Size([512])\n",
      "layer4_a.2.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4_a.2.bn1.running_mean \t torch.Size([512])\n",
      "layer4_a.2.bn1.running_var \t torch.Size([512])\n",
      "layer4_a.2.bn1.weight \t torch.Size([512])\n",
      "layer4_a.2.bn1.bias \t torch.Size([512])\n",
      "layer4_a.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4_a.2.bn2.running_mean \t torch.Size([512])\n",
      "layer4_a.2.bn2.running_var \t torch.Size([512])\n",
      "layer4_a.2.bn2.weight \t torch.Size([512])\n",
      "layer4_a.2.bn2.bias \t torch.Size([512])\n",
      "conv1_b.weight \t torch.Size([64, 20, 7, 7])\n",
      "bn1_b.running_mean \t torch.Size([64])\n",
      "bn1_b.running_var \t torch.Size([64])\n",
      "bn1_b.weight \t torch.Size([64])\n",
      "bn1_b.bias \t torch.Size([64])\n",
      "layer1_b.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1_b.0.bn1.running_mean \t torch.Size([64])\n",
      "layer1_b.0.bn1.running_var \t torch.Size([64])\n",
      "layer1_b.0.bn1.weight \t torch.Size([64])\n",
      "layer1_b.0.bn1.bias \t torch.Size([64])\n",
      "layer1_b.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1_b.0.bn2.running_mean \t torch.Size([64])\n",
      "layer1_b.0.bn2.running_var \t torch.Size([64])\n",
      "layer1_b.0.bn2.weight \t torch.Size([64])\n",
      "layer1_b.0.bn2.bias \t torch.Size([64])\n",
      "layer1_b.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1_b.1.bn1.running_mean \t torch.Size([64])\n",
      "layer1_b.1.bn1.running_var \t torch.Size([64])\n",
      "layer1_b.1.bn1.weight \t torch.Size([64])\n",
      "layer1_b.1.bn1.bias \t torch.Size([64])\n",
      "layer1_b.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1_b.1.bn2.running_mean \t torch.Size([64])\n",
      "layer1_b.1.bn2.running_var \t torch.Size([64])\n",
      "layer1_b.1.bn2.weight \t torch.Size([64])\n",
      "layer1_b.1.bn2.bias \t torch.Size([64])\n",
      "layer1_b.2.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1_b.2.bn1.running_mean \t torch.Size([64])\n",
      "layer1_b.2.bn1.running_var \t torch.Size([64])\n",
      "layer1_b.2.bn1.weight \t torch.Size([64])\n",
      "layer1_b.2.bn1.bias \t torch.Size([64])\n",
      "layer1_b.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "layer1_b.2.bn2.running_mean \t torch.Size([64])\n",
      "layer1_b.2.bn2.running_var \t torch.Size([64])\n",
      "layer1_b.2.bn2.weight \t torch.Size([64])\n",
      "layer1_b.2.bn2.bias \t torch.Size([64])\n",
      "layer2_b.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "layer2_b.0.bn1.running_mean \t torch.Size([128])\n",
      "layer2_b.0.bn1.running_var \t torch.Size([128])\n",
      "layer2_b.0.bn1.weight \t torch.Size([128])\n",
      "layer2_b.0.bn1.bias \t torch.Size([128])\n",
      "layer2_b.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_b.0.bn2.running_mean \t torch.Size([128])\n",
      "layer2_b.0.bn2.running_var \t torch.Size([128])\n",
      "layer2_b.0.bn2.weight \t torch.Size([128])\n",
      "layer2_b.0.bn2.bias \t torch.Size([128])\n",
      "layer2_b.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "layer2_b.0.downsample.1.running_mean \t torch.Size([128])\n",
      "layer2_b.0.downsample.1.running_var \t torch.Size([128])\n",
      "layer2_b.0.downsample.1.weight \t torch.Size([128])\n",
      "layer2_b.0.downsample.1.bias \t torch.Size([128])\n",
      "layer2_b.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_b.1.bn1.running_mean \t torch.Size([128])\n",
      "layer2_b.1.bn1.running_var \t torch.Size([128])\n",
      "layer2_b.1.bn1.weight \t torch.Size([128])\n",
      "layer2_b.1.bn1.bias \t torch.Size([128])\n",
      "layer2_b.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_b.1.bn2.running_mean \t torch.Size([128])\n",
      "layer2_b.1.bn2.running_var \t torch.Size([128])\n",
      "layer2_b.1.bn2.weight \t torch.Size([128])\n",
      "layer2_b.1.bn2.bias \t torch.Size([128])\n",
      "layer2_b.2.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_b.2.bn1.running_mean \t torch.Size([128])\n",
      "layer2_b.2.bn1.running_var \t torch.Size([128])\n",
      "layer2_b.2.bn1.weight \t torch.Size([128])\n",
      "layer2_b.2.bn1.bias \t torch.Size([128])\n",
      "layer2_b.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_b.2.bn2.running_mean \t torch.Size([128])\n",
      "layer2_b.2.bn2.running_var \t torch.Size([128])\n",
      "layer2_b.2.bn2.weight \t torch.Size([128])\n",
      "layer2_b.2.bn2.bias \t torch.Size([128])\n",
      "layer2_b.3.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_b.3.bn1.running_mean \t torch.Size([128])\n",
      "layer2_b.3.bn1.running_var \t torch.Size([128])\n",
      "layer2_b.3.bn1.weight \t torch.Size([128])\n",
      "layer2_b.3.bn1.bias \t torch.Size([128])\n",
      "layer2_b.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "layer2_b.3.bn2.running_mean \t torch.Size([128])\n",
      "layer2_b.3.bn2.running_var \t torch.Size([128])\n",
      "layer2_b.3.bn2.weight \t torch.Size([128])\n",
      "layer2_b.3.bn2.bias \t torch.Size([128])\n",
      "layer3_b.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "layer3_b.0.bn1.running_mean \t torch.Size([256])\n",
      "layer3_b.0.bn1.running_var \t torch.Size([256])\n",
      "layer3_b.0.bn1.weight \t torch.Size([256])\n",
      "layer3_b.0.bn1.bias \t torch.Size([256])\n",
      "layer3_b.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_b.0.bn2.running_mean \t torch.Size([256])\n",
      "layer3_b.0.bn2.running_var \t torch.Size([256])\n",
      "layer3_b.0.bn2.weight \t torch.Size([256])\n",
      "layer3_b.0.bn2.bias \t torch.Size([256])\n",
      "layer3_b.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "layer3_b.0.downsample.1.running_mean \t torch.Size([256])\n",
      "layer3_b.0.downsample.1.running_var \t torch.Size([256])\n",
      "layer3_b.0.downsample.1.weight \t torch.Size([256])\n",
      "layer3_b.0.downsample.1.bias \t torch.Size([256])\n",
      "layer3_b.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_b.1.bn1.running_mean \t torch.Size([256])\n",
      "layer3_b.1.bn1.running_var \t torch.Size([256])\n",
      "layer3_b.1.bn1.weight \t torch.Size([256])\n",
      "layer3_b.1.bn1.bias \t torch.Size([256])\n",
      "layer3_b.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_b.1.bn2.running_mean \t torch.Size([256])\n",
      "layer3_b.1.bn2.running_var \t torch.Size([256])\n",
      "layer3_b.1.bn2.weight \t torch.Size([256])\n",
      "layer3_b.1.bn2.bias \t torch.Size([256])\n",
      "layer3_b.2.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_b.2.bn1.running_mean \t torch.Size([256])\n",
      "layer3_b.2.bn1.running_var \t torch.Size([256])\n",
      "layer3_b.2.bn1.weight \t torch.Size([256])\n",
      "layer3_b.2.bn1.bias \t torch.Size([256])\n",
      "layer3_b.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_b.2.bn2.running_mean \t torch.Size([256])\n",
      "layer3_b.2.bn2.running_var \t torch.Size([256])\n",
      "layer3_b.2.bn2.weight \t torch.Size([256])\n",
      "layer3_b.2.bn2.bias \t torch.Size([256])\n",
      "layer3_b.3.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_b.3.bn1.running_mean \t torch.Size([256])\n",
      "layer3_b.3.bn1.running_var \t torch.Size([256])\n",
      "layer3_b.3.bn1.weight \t torch.Size([256])\n",
      "layer3_b.3.bn1.bias \t torch.Size([256])\n",
      "layer3_b.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_b.3.bn2.running_mean \t torch.Size([256])\n",
      "layer3_b.3.bn2.running_var \t torch.Size([256])\n",
      "layer3_b.3.bn2.weight \t torch.Size([256])\n",
      "layer3_b.3.bn2.bias \t torch.Size([256])\n",
      "layer3_b.4.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_b.4.bn1.running_mean \t torch.Size([256])\n",
      "layer3_b.4.bn1.running_var \t torch.Size([256])\n",
      "layer3_b.4.bn1.weight \t torch.Size([256])\n",
      "layer3_b.4.bn1.bias \t torch.Size([256])\n",
      "layer3_b.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_b.4.bn2.running_mean \t torch.Size([256])\n",
      "layer3_b.4.bn2.running_var \t torch.Size([256])\n",
      "layer3_b.4.bn2.weight \t torch.Size([256])\n",
      "layer3_b.4.bn2.bias \t torch.Size([256])\n",
      "layer3_b.5.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_b.5.bn1.running_mean \t torch.Size([256])\n",
      "layer3_b.5.bn1.running_var \t torch.Size([256])\n",
      "layer3_b.5.bn1.weight \t torch.Size([256])\n",
      "layer3_b.5.bn1.bias \t torch.Size([256])\n",
      "layer3_b.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "layer3_b.5.bn2.running_mean \t torch.Size([256])\n",
      "layer3_b.5.bn2.running_var \t torch.Size([256])\n",
      "layer3_b.5.bn2.weight \t torch.Size([256])\n",
      "layer3_b.5.bn2.bias \t torch.Size([256])\n",
      "layer4_b.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "layer4_b.0.bn1.running_mean \t torch.Size([512])\n",
      "layer4_b.0.bn1.running_var \t torch.Size([512])\n",
      "layer4_b.0.bn1.weight \t torch.Size([512])\n",
      "layer4_b.0.bn1.bias \t torch.Size([512])\n",
      "layer4_b.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4_b.0.bn2.running_mean \t torch.Size([512])\n",
      "layer4_b.0.bn2.running_var \t torch.Size([512])\n",
      "layer4_b.0.bn2.weight \t torch.Size([512])\n",
      "layer4_b.0.bn2.bias \t torch.Size([512])\n",
      "layer4_b.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "layer4_b.0.downsample.1.running_mean \t torch.Size([512])\n",
      "layer4_b.0.downsample.1.running_var \t torch.Size([512])\n",
      "layer4_b.0.downsample.1.weight \t torch.Size([512])\n",
      "layer4_b.0.downsample.1.bias \t torch.Size([512])\n",
      "layer4_b.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4_b.1.bn1.running_mean \t torch.Size([512])\n",
      "layer4_b.1.bn1.running_var \t torch.Size([512])\n",
      "layer4_b.1.bn1.weight \t torch.Size([512])\n",
      "layer4_b.1.bn1.bias \t torch.Size([512])\n",
      "layer4_b.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4_b.1.bn2.running_mean \t torch.Size([512])\n",
      "layer4_b.1.bn2.running_var \t torch.Size([512])\n",
      "layer4_b.1.bn2.weight \t torch.Size([512])\n",
      "layer4_b.1.bn2.bias \t torch.Size([512])\n",
      "layer4_b.2.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4_b.2.bn1.running_mean \t torch.Size([512])\n",
      "layer4_b.2.bn1.running_var \t torch.Size([512])\n",
      "layer4_b.2.bn1.weight \t torch.Size([512])\n",
      "layer4_b.2.bn1.bias \t torch.Size([512])\n",
      "layer4_b.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "layer4_b.2.bn2.running_mean \t torch.Size([512])\n",
      "layer4_b.2.bn2.running_var \t torch.Size([512])\n",
      "layer4_b.2.bn2.weight \t torch.Size([512])\n",
      "layer4_b.2.bn2.bias \t torch.Size([512])\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "update new_pretrained_dict's state_dict: 360\n",
      "0 conv1_a.weight \t torch.Size([64, 3, 7, 7])\n",
      "1 bn1_a.running_mean \t torch.Size([64])\n",
      "2 bn1_a.running_var \t torch.Size([64])\n",
      "3 bn1_a.weight \t torch.Size([64])\n",
      "4 bn1_a.bias \t torch.Size([64])\n",
      "5 layer1_a.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "6 layer1_a.0.bn1.running_mean \t torch.Size([64])\n",
      "7 layer1_a.0.bn1.running_var \t torch.Size([64])\n",
      "8 layer1_a.0.bn1.weight \t torch.Size([64])\n",
      "9 layer1_a.0.bn1.bias \t torch.Size([64])\n",
      "10 layer1_a.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "11 layer1_a.0.bn2.running_mean \t torch.Size([64])\n",
      "12 layer1_a.0.bn2.running_var \t torch.Size([64])\n",
      "13 layer1_a.0.bn2.weight \t torch.Size([64])\n",
      "14 layer1_a.0.bn2.bias \t torch.Size([64])\n",
      "15 layer1_a.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "16 layer1_a.1.bn1.running_mean \t torch.Size([64])\n",
      "17 layer1_a.1.bn1.running_var \t torch.Size([64])\n",
      "18 layer1_a.1.bn1.weight \t torch.Size([64])\n",
      "19 layer1_a.1.bn1.bias \t torch.Size([64])\n",
      "20 layer1_a.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "21 layer1_a.1.bn2.running_mean \t torch.Size([64])\n",
      "22 layer1_a.1.bn2.running_var \t torch.Size([64])\n",
      "23 layer1_a.1.bn2.weight \t torch.Size([64])\n",
      "24 layer1_a.1.bn2.bias \t torch.Size([64])\n",
      "25 layer1_a.2.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "26 layer1_a.2.bn1.running_mean \t torch.Size([64])\n",
      "27 layer1_a.2.bn1.running_var \t torch.Size([64])\n",
      "28 layer1_a.2.bn1.weight \t torch.Size([64])\n",
      "29 layer1_a.2.bn1.bias \t torch.Size([64])\n",
      "30 layer1_a.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "31 layer1_a.2.bn2.running_mean \t torch.Size([64])\n",
      "32 layer1_a.2.bn2.running_var \t torch.Size([64])\n",
      "33 layer1_a.2.bn2.weight \t torch.Size([64])\n",
      "34 layer1_a.2.bn2.bias \t torch.Size([64])\n",
      "35 layer2_a.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "36 layer2_a.0.bn1.running_mean \t torch.Size([128])\n",
      "37 layer2_a.0.bn1.running_var \t torch.Size([128])\n",
      "38 layer2_a.0.bn1.weight \t torch.Size([128])\n",
      "39 layer2_a.0.bn1.bias \t torch.Size([128])\n",
      "40 layer2_a.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "41 layer2_a.0.bn2.running_mean \t torch.Size([128])\n",
      "42 layer2_a.0.bn2.running_var \t torch.Size([128])\n",
      "43 layer2_a.0.bn2.weight \t torch.Size([128])\n",
      "44 layer2_a.0.bn2.bias \t torch.Size([128])\n",
      "45 layer2_a.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "46 layer2_a.0.downsample.1.running_mean \t torch.Size([128])\n",
      "47 layer2_a.0.downsample.1.running_var \t torch.Size([128])\n",
      "48 layer2_a.0.downsample.1.weight \t torch.Size([128])\n",
      "49 layer2_a.0.downsample.1.bias \t torch.Size([128])\n",
      "50 layer2_a.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "51 layer2_a.1.bn1.running_mean \t torch.Size([128])\n",
      "52 layer2_a.1.bn1.running_var \t torch.Size([128])\n",
      "53 layer2_a.1.bn1.weight \t torch.Size([128])\n",
      "54 layer2_a.1.bn1.bias \t torch.Size([128])\n",
      "55 layer2_a.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "56 layer2_a.1.bn2.running_mean \t torch.Size([128])\n",
      "57 layer2_a.1.bn2.running_var \t torch.Size([128])\n",
      "58 layer2_a.1.bn2.weight \t torch.Size([128])\n",
      "59 layer2_a.1.bn2.bias \t torch.Size([128])\n",
      "60 layer2_a.2.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "61 layer2_a.2.bn1.running_mean \t torch.Size([128])\n",
      "62 layer2_a.2.bn1.running_var \t torch.Size([128])\n",
      "63 layer2_a.2.bn1.weight \t torch.Size([128])\n",
      "64 layer2_a.2.bn1.bias \t torch.Size([128])\n",
      "65 layer2_a.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "66 layer2_a.2.bn2.running_mean \t torch.Size([128])\n",
      "67 layer2_a.2.bn2.running_var \t torch.Size([128])\n",
      "68 layer2_a.2.bn2.weight \t torch.Size([128])\n",
      "69 layer2_a.2.bn2.bias \t torch.Size([128])\n",
      "70 layer2_a.3.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "71 layer2_a.3.bn1.running_mean \t torch.Size([128])\n",
      "72 layer2_a.3.bn1.running_var \t torch.Size([128])\n",
      "73 layer2_a.3.bn1.weight \t torch.Size([128])\n",
      "74 layer2_a.3.bn1.bias \t torch.Size([128])\n",
      "75 layer2_a.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "76 layer2_a.3.bn2.running_mean \t torch.Size([128])\n",
      "77 layer2_a.3.bn2.running_var \t torch.Size([128])\n",
      "78 layer2_a.3.bn2.weight \t torch.Size([128])\n",
      "79 layer2_a.3.bn2.bias \t torch.Size([128])\n",
      "80 layer3_a.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "81 layer3_a.0.bn1.running_mean \t torch.Size([256])\n",
      "82 layer3_a.0.bn1.running_var \t torch.Size([256])\n",
      "83 layer3_a.0.bn1.weight \t torch.Size([256])\n",
      "84 layer3_a.0.bn1.bias \t torch.Size([256])\n",
      "85 layer3_a.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "86 layer3_a.0.bn2.running_mean \t torch.Size([256])\n",
      "87 layer3_a.0.bn2.running_var \t torch.Size([256])\n",
      "88 layer3_a.0.bn2.weight \t torch.Size([256])\n",
      "89 layer3_a.0.bn2.bias \t torch.Size([256])\n",
      "90 layer3_a.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "91 layer3_a.0.downsample.1.running_mean \t torch.Size([256])\n",
      "92 layer3_a.0.downsample.1.running_var \t torch.Size([256])\n",
      "93 layer3_a.0.downsample.1.weight \t torch.Size([256])\n",
      "94 layer3_a.0.downsample.1.bias \t torch.Size([256])\n",
      "95 layer3_a.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "96 layer3_a.1.bn1.running_mean \t torch.Size([256])\n",
      "97 layer3_a.1.bn1.running_var \t torch.Size([256])\n",
      "98 layer3_a.1.bn1.weight \t torch.Size([256])\n",
      "99 layer3_a.1.bn1.bias \t torch.Size([256])\n",
      "100 layer3_a.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "101 layer3_a.1.bn2.running_mean \t torch.Size([256])\n",
      "102 layer3_a.1.bn2.running_var \t torch.Size([256])\n",
      "103 layer3_a.1.bn2.weight \t torch.Size([256])\n",
      "104 layer3_a.1.bn2.bias \t torch.Size([256])\n",
      "105 layer3_a.2.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "106 layer3_a.2.bn1.running_mean \t torch.Size([256])\n",
      "107 layer3_a.2.bn1.running_var \t torch.Size([256])\n",
      "108 layer3_a.2.bn1.weight \t torch.Size([256])\n",
      "109 layer3_a.2.bn1.bias \t torch.Size([256])\n",
      "110 layer3_a.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "111 layer3_a.2.bn2.running_mean \t torch.Size([256])\n",
      "112 layer3_a.2.bn2.running_var \t torch.Size([256])\n",
      "113 layer3_a.2.bn2.weight \t torch.Size([256])\n",
      "114 layer3_a.2.bn2.bias \t torch.Size([256])\n",
      "115 layer3_a.3.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "116 layer3_a.3.bn1.running_mean \t torch.Size([256])\n",
      "117 layer3_a.3.bn1.running_var \t torch.Size([256])\n",
      "118 layer3_a.3.bn1.weight \t torch.Size([256])\n",
      "119 layer3_a.3.bn1.bias \t torch.Size([256])\n",
      "120 layer3_a.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "121 layer3_a.3.bn2.running_mean \t torch.Size([256])\n",
      "122 layer3_a.3.bn2.running_var \t torch.Size([256])\n",
      "123 layer3_a.3.bn2.weight \t torch.Size([256])\n",
      "124 layer3_a.3.bn2.bias \t torch.Size([256])\n",
      "125 layer3_a.4.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "126 layer3_a.4.bn1.running_mean \t torch.Size([256])\n",
      "127 layer3_a.4.bn1.running_var \t torch.Size([256])\n",
      "128 layer3_a.4.bn1.weight \t torch.Size([256])\n",
      "129 layer3_a.4.bn1.bias \t torch.Size([256])\n",
      "130 layer3_a.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "131 layer3_a.4.bn2.running_mean \t torch.Size([256])\n",
      "132 layer3_a.4.bn2.running_var \t torch.Size([256])\n",
      "133 layer3_a.4.bn2.weight \t torch.Size([256])\n",
      "134 layer3_a.4.bn2.bias \t torch.Size([256])\n",
      "135 layer3_a.5.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "136 layer3_a.5.bn1.running_mean \t torch.Size([256])\n",
      "137 layer3_a.5.bn1.running_var \t torch.Size([256])\n",
      "138 layer3_a.5.bn1.weight \t torch.Size([256])\n",
      "139 layer3_a.5.bn1.bias \t torch.Size([256])\n",
      "140 layer3_a.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "141 layer3_a.5.bn2.running_mean \t torch.Size([256])\n",
      "142 layer3_a.5.bn2.running_var \t torch.Size([256])\n",
      "143 layer3_a.5.bn2.weight \t torch.Size([256])\n",
      "144 layer3_a.5.bn2.bias \t torch.Size([256])\n",
      "145 layer4_a.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "146 layer4_a.0.bn1.running_mean \t torch.Size([512])\n",
      "147 layer4_a.0.bn1.running_var \t torch.Size([512])\n",
      "148 layer4_a.0.bn1.weight \t torch.Size([512])\n",
      "149 layer4_a.0.bn1.bias \t torch.Size([512])\n",
      "150 layer4_a.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "151 layer4_a.0.bn2.running_mean \t torch.Size([512])\n",
      "152 layer4_a.0.bn2.running_var \t torch.Size([512])\n",
      "153 layer4_a.0.bn2.weight \t torch.Size([512])\n",
      "154 layer4_a.0.bn2.bias \t torch.Size([512])\n",
      "155 layer4_a.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "156 layer4_a.0.downsample.1.running_mean \t torch.Size([512])\n",
      "157 layer4_a.0.downsample.1.running_var \t torch.Size([512])\n",
      "158 layer4_a.0.downsample.1.weight \t torch.Size([512])\n",
      "159 layer4_a.0.downsample.1.bias \t torch.Size([512])\n",
      "160 layer4_a.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "161 layer4_a.1.bn1.running_mean \t torch.Size([512])\n",
      "162 layer4_a.1.bn1.running_var \t torch.Size([512])\n",
      "163 layer4_a.1.bn1.weight \t torch.Size([512])\n",
      "164 layer4_a.1.bn1.bias \t torch.Size([512])\n",
      "165 layer4_a.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "166 layer4_a.1.bn2.running_mean \t torch.Size([512])\n",
      "167 layer4_a.1.bn2.running_var \t torch.Size([512])\n",
      "168 layer4_a.1.bn2.weight \t torch.Size([512])\n",
      "169 layer4_a.1.bn2.bias \t torch.Size([512])\n",
      "170 layer4_a.2.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "171 layer4_a.2.bn1.running_mean \t torch.Size([512])\n",
      "172 layer4_a.2.bn1.running_var \t torch.Size([512])\n",
      "173 layer4_a.2.bn1.weight \t torch.Size([512])\n",
      "174 layer4_a.2.bn1.bias \t torch.Size([512])\n",
      "175 layer4_a.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "176 layer4_a.2.bn2.running_mean \t torch.Size([512])\n",
      "177 layer4_a.2.bn2.running_var \t torch.Size([512])\n",
      "178 layer4_a.2.bn2.weight \t torch.Size([512])\n",
      "179 layer4_a.2.bn2.bias \t torch.Size([512])\n",
      "180 conv1_b.weight \t torch.Size([64, 20, 7, 7])\n",
      "181 bn1_b.running_mean \t torch.Size([64])\n",
      "182 bn1_b.running_var \t torch.Size([64])\n",
      "183 bn1_b.weight \t torch.Size([64])\n",
      "184 bn1_b.bias \t torch.Size([64])\n",
      "185 layer1_b.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "186 layer1_b.0.bn1.running_mean \t torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 layer1_b.0.bn1.running_var \t torch.Size([64])\n",
      "188 layer1_b.0.bn1.weight \t torch.Size([64])\n",
      "189 layer1_b.0.bn1.bias \t torch.Size([64])\n",
      "190 layer1_b.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "191 layer1_b.0.bn2.running_mean \t torch.Size([64])\n",
      "192 layer1_b.0.bn2.running_var \t torch.Size([64])\n",
      "193 layer1_b.0.bn2.weight \t torch.Size([64])\n",
      "194 layer1_b.0.bn2.bias \t torch.Size([64])\n",
      "195 layer1_b.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "196 layer1_b.1.bn1.running_mean \t torch.Size([64])\n",
      "197 layer1_b.1.bn1.running_var \t torch.Size([64])\n",
      "198 layer1_b.1.bn1.weight \t torch.Size([64])\n",
      "199 layer1_b.1.bn1.bias \t torch.Size([64])\n",
      "200 layer1_b.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "201 layer1_b.1.bn2.running_mean \t torch.Size([64])\n",
      "202 layer1_b.1.bn2.running_var \t torch.Size([64])\n",
      "203 layer1_b.1.bn2.weight \t torch.Size([64])\n",
      "204 layer1_b.1.bn2.bias \t torch.Size([64])\n",
      "205 layer1_b.2.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "206 layer1_b.2.bn1.running_mean \t torch.Size([64])\n",
      "207 layer1_b.2.bn1.running_var \t torch.Size([64])\n",
      "208 layer1_b.2.bn1.weight \t torch.Size([64])\n",
      "209 layer1_b.2.bn1.bias \t torch.Size([64])\n",
      "210 layer1_b.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "211 layer1_b.2.bn2.running_mean \t torch.Size([64])\n",
      "212 layer1_b.2.bn2.running_var \t torch.Size([64])\n",
      "213 layer1_b.2.bn2.weight \t torch.Size([64])\n",
      "214 layer1_b.2.bn2.bias \t torch.Size([64])\n",
      "215 layer2_b.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "216 layer2_b.0.bn1.running_mean \t torch.Size([128])\n",
      "217 layer2_b.0.bn1.running_var \t torch.Size([128])\n",
      "218 layer2_b.0.bn1.weight \t torch.Size([128])\n",
      "219 layer2_b.0.bn1.bias \t torch.Size([128])\n",
      "220 layer2_b.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "221 layer2_b.0.bn2.running_mean \t torch.Size([128])\n",
      "222 layer2_b.0.bn2.running_var \t torch.Size([128])\n",
      "223 layer2_b.0.bn2.weight \t torch.Size([128])\n",
      "224 layer2_b.0.bn2.bias \t torch.Size([128])\n",
      "225 layer2_b.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "226 layer2_b.0.downsample.1.running_mean \t torch.Size([128])\n",
      "227 layer2_b.0.downsample.1.running_var \t torch.Size([128])\n",
      "228 layer2_b.0.downsample.1.weight \t torch.Size([128])\n",
      "229 layer2_b.0.downsample.1.bias \t torch.Size([128])\n",
      "230 layer2_b.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "231 layer2_b.1.bn1.running_mean \t torch.Size([128])\n",
      "232 layer2_b.1.bn1.running_var \t torch.Size([128])\n",
      "233 layer2_b.1.bn1.weight \t torch.Size([128])\n",
      "234 layer2_b.1.bn1.bias \t torch.Size([128])\n",
      "235 layer2_b.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "236 layer2_b.1.bn2.running_mean \t torch.Size([128])\n",
      "237 layer2_b.1.bn2.running_var \t torch.Size([128])\n",
      "238 layer2_b.1.bn2.weight \t torch.Size([128])\n",
      "239 layer2_b.1.bn2.bias \t torch.Size([128])\n",
      "240 layer2_b.2.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "241 layer2_b.2.bn1.running_mean \t torch.Size([128])\n",
      "242 layer2_b.2.bn1.running_var \t torch.Size([128])\n",
      "243 layer2_b.2.bn1.weight \t torch.Size([128])\n",
      "244 layer2_b.2.bn1.bias \t torch.Size([128])\n",
      "245 layer2_b.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "246 layer2_b.2.bn2.running_mean \t torch.Size([128])\n",
      "247 layer2_b.2.bn2.running_var \t torch.Size([128])\n",
      "248 layer2_b.2.bn2.weight \t torch.Size([128])\n",
      "249 layer2_b.2.bn2.bias \t torch.Size([128])\n",
      "250 layer2_b.3.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "251 layer2_b.3.bn1.running_mean \t torch.Size([128])\n",
      "252 layer2_b.3.bn1.running_var \t torch.Size([128])\n",
      "253 layer2_b.3.bn1.weight \t torch.Size([128])\n",
      "254 layer2_b.3.bn1.bias \t torch.Size([128])\n",
      "255 layer2_b.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "256 layer2_b.3.bn2.running_mean \t torch.Size([128])\n",
      "257 layer2_b.3.bn2.running_var \t torch.Size([128])\n",
      "258 layer2_b.3.bn2.weight \t torch.Size([128])\n",
      "259 layer2_b.3.bn2.bias \t torch.Size([128])\n",
      "260 layer3_b.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "261 layer3_b.0.bn1.running_mean \t torch.Size([256])\n",
      "262 layer3_b.0.bn1.running_var \t torch.Size([256])\n",
      "263 layer3_b.0.bn1.weight \t torch.Size([256])\n",
      "264 layer3_b.0.bn1.bias \t torch.Size([256])\n",
      "265 layer3_b.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "266 layer3_b.0.bn2.running_mean \t torch.Size([256])\n",
      "267 layer3_b.0.bn2.running_var \t torch.Size([256])\n",
      "268 layer3_b.0.bn2.weight \t torch.Size([256])\n",
      "269 layer3_b.0.bn2.bias \t torch.Size([256])\n",
      "270 layer3_b.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "271 layer3_b.0.downsample.1.running_mean \t torch.Size([256])\n",
      "272 layer3_b.0.downsample.1.running_var \t torch.Size([256])\n",
      "273 layer3_b.0.downsample.1.weight \t torch.Size([256])\n",
      "274 layer3_b.0.downsample.1.bias \t torch.Size([256])\n",
      "275 layer3_b.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "276 layer3_b.1.bn1.running_mean \t torch.Size([256])\n",
      "277 layer3_b.1.bn1.running_var \t torch.Size([256])\n",
      "278 layer3_b.1.bn1.weight \t torch.Size([256])\n",
      "279 layer3_b.1.bn1.bias \t torch.Size([256])\n",
      "280 layer3_b.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "281 layer3_b.1.bn2.running_mean \t torch.Size([256])\n",
      "282 layer3_b.1.bn2.running_var \t torch.Size([256])\n",
      "283 layer3_b.1.bn2.weight \t torch.Size([256])\n",
      "284 layer3_b.1.bn2.bias \t torch.Size([256])\n",
      "285 layer3_b.2.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "286 layer3_b.2.bn1.running_mean \t torch.Size([256])\n",
      "287 layer3_b.2.bn1.running_var \t torch.Size([256])\n",
      "288 layer3_b.2.bn1.weight \t torch.Size([256])\n",
      "289 layer3_b.2.bn1.bias \t torch.Size([256])\n",
      "290 layer3_b.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "291 layer3_b.2.bn2.running_mean \t torch.Size([256])\n",
      "292 layer3_b.2.bn2.running_var \t torch.Size([256])\n",
      "293 layer3_b.2.bn2.weight \t torch.Size([256])\n",
      "294 layer3_b.2.bn2.bias \t torch.Size([256])\n",
      "295 layer3_b.3.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "296 layer3_b.3.bn1.running_mean \t torch.Size([256])\n",
      "297 layer3_b.3.bn1.running_var \t torch.Size([256])\n",
      "298 layer3_b.3.bn1.weight \t torch.Size([256])\n",
      "299 layer3_b.3.bn1.bias \t torch.Size([256])\n",
      "300 layer3_b.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "301 layer3_b.3.bn2.running_mean \t torch.Size([256])\n",
      "302 layer3_b.3.bn2.running_var \t torch.Size([256])\n",
      "303 layer3_b.3.bn2.weight \t torch.Size([256])\n",
      "304 layer3_b.3.bn2.bias \t torch.Size([256])\n",
      "305 layer3_b.4.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "306 layer3_b.4.bn1.running_mean \t torch.Size([256])\n",
      "307 layer3_b.4.bn1.running_var \t torch.Size([256])\n",
      "308 layer3_b.4.bn1.weight \t torch.Size([256])\n",
      "309 layer3_b.4.bn1.bias \t torch.Size([256])\n",
      "310 layer3_b.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "311 layer3_b.4.bn2.running_mean \t torch.Size([256])\n",
      "312 layer3_b.4.bn2.running_var \t torch.Size([256])\n",
      "313 layer3_b.4.bn2.weight \t torch.Size([256])\n",
      "314 layer3_b.4.bn2.bias \t torch.Size([256])\n",
      "315 layer3_b.5.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "316 layer3_b.5.bn1.running_mean \t torch.Size([256])\n",
      "317 layer3_b.5.bn1.running_var \t torch.Size([256])\n",
      "318 layer3_b.5.bn1.weight \t torch.Size([256])\n",
      "319 layer3_b.5.bn1.bias \t torch.Size([256])\n",
      "320 layer3_b.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "321 layer3_b.5.bn2.running_mean \t torch.Size([256])\n",
      "322 layer3_b.5.bn2.running_var \t torch.Size([256])\n",
      "323 layer3_b.5.bn2.weight \t torch.Size([256])\n",
      "324 layer3_b.5.bn2.bias \t torch.Size([256])\n",
      "325 layer4_b.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "326 layer4_b.0.bn1.running_mean \t torch.Size([512])\n",
      "327 layer4_b.0.bn1.running_var \t torch.Size([512])\n",
      "328 layer4_b.0.bn1.weight \t torch.Size([512])\n",
      "329 layer4_b.0.bn1.bias \t torch.Size([512])\n",
      "330 layer4_b.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "331 layer4_b.0.bn2.running_mean \t torch.Size([512])\n",
      "332 layer4_b.0.bn2.running_var \t torch.Size([512])\n",
      "333 layer4_b.0.bn2.weight \t torch.Size([512])\n",
      "334 layer4_b.0.bn2.bias \t torch.Size([512])\n",
      "335 layer4_b.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "336 layer4_b.0.downsample.1.running_mean \t torch.Size([512])\n",
      "337 layer4_b.0.downsample.1.running_var \t torch.Size([512])\n",
      "338 layer4_b.0.downsample.1.weight \t torch.Size([512])\n",
      "339 layer4_b.0.downsample.1.bias \t torch.Size([512])\n",
      "340 layer4_b.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "341 layer4_b.1.bn1.running_mean \t torch.Size([512])\n",
      "342 layer4_b.1.bn1.running_var \t torch.Size([512])\n",
      "343 layer4_b.1.bn1.weight \t torch.Size([512])\n",
      "344 layer4_b.1.bn1.bias \t torch.Size([512])\n",
      "345 layer4_b.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "346 layer4_b.1.bn2.running_mean \t torch.Size([512])\n",
      "347 layer4_b.1.bn2.running_var \t torch.Size([512])\n",
      "348 layer4_b.1.bn2.weight \t torch.Size([512])\n",
      "349 layer4_b.1.bn2.bias \t torch.Size([512])\n",
      "350 layer4_b.2.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "351 layer4_b.2.bn1.running_mean \t torch.Size([512])\n",
      "352 layer4_b.2.bn1.running_var \t torch.Size([512])\n",
      "353 layer4_b.2.bn1.weight \t torch.Size([512])\n",
      "354 layer4_b.2.bn1.bias \t torch.Size([512])\n",
      "355 layer4_b.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "356 layer4_b.2.bn2.running_mean \t torch.Size([512])\n",
      "357 layer4_b.2.bn2.running_var \t torch.Size([512])\n",
      "358 layer4_b.2.bn2.weight \t torch.Size([512])\n",
      "359 layer4_b.2.bn2.bias \t torch.Size([512])\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "update model_dict's state_dict: 439\n",
      "0 conv1_a.weight \t torch.Size([64, 3, 7, 7])\n",
      "1 bn1_a.weight \t torch.Size([64])\n",
      "2 bn1_a.bias \t torch.Size([64])\n",
      "3 bn1_a.running_mean \t torch.Size([64])\n",
      "4 bn1_a.running_var \t torch.Size([64])\n",
      "5 bn1_a.num_batches_tracked \t torch.Size([])\n",
      "6 layer1_a.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "7 layer1_a.0.bn1.weight \t torch.Size([64])\n",
      "8 layer1_a.0.bn1.bias \t torch.Size([64])\n",
      "9 layer1_a.0.bn1.running_mean \t torch.Size([64])\n",
      "10 layer1_a.0.bn1.running_var \t torch.Size([64])\n",
      "11 layer1_a.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "12 layer1_a.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "13 layer1_a.0.bn2.weight \t torch.Size([64])\n",
      "14 layer1_a.0.bn2.bias \t torch.Size([64])\n",
      "15 layer1_a.0.bn2.running_mean \t torch.Size([64])\n",
      "16 layer1_a.0.bn2.running_var \t torch.Size([64])\n",
      "17 layer1_a.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "18 layer1_a.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "19 layer1_a.1.bn1.weight \t torch.Size([64])\n",
      "20 layer1_a.1.bn1.bias \t torch.Size([64])\n",
      "21 layer1_a.1.bn1.running_mean \t torch.Size([64])\n",
      "22 layer1_a.1.bn1.running_var \t torch.Size([64])\n",
      "23 layer1_a.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "24 layer1_a.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "25 layer1_a.1.bn2.weight \t torch.Size([64])\n",
      "26 layer1_a.1.bn2.bias \t torch.Size([64])\n",
      "27 layer1_a.1.bn2.running_mean \t torch.Size([64])\n",
      "28 layer1_a.1.bn2.running_var \t torch.Size([64])\n",
      "29 layer1_a.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "30 layer1_a.2.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "31 layer1_a.2.bn1.weight \t torch.Size([64])\n",
      "32 layer1_a.2.bn1.bias \t torch.Size([64])\n",
      "33 layer1_a.2.bn1.running_mean \t torch.Size([64])\n",
      "34 layer1_a.2.bn1.running_var \t torch.Size([64])\n",
      "35 layer1_a.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "36 layer1_a.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "37 layer1_a.2.bn2.weight \t torch.Size([64])\n",
      "38 layer1_a.2.bn2.bias \t torch.Size([64])\n",
      "39 layer1_a.2.bn2.running_mean \t torch.Size([64])\n",
      "40 layer1_a.2.bn2.running_var \t torch.Size([64])\n",
      "41 layer1_a.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "42 layer2_a.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "43 layer2_a.0.bn1.weight \t torch.Size([128])\n",
      "44 layer2_a.0.bn1.bias \t torch.Size([128])\n",
      "45 layer2_a.0.bn1.running_mean \t torch.Size([128])\n",
      "46 layer2_a.0.bn1.running_var \t torch.Size([128])\n",
      "47 layer2_a.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "48 layer2_a.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "49 layer2_a.0.bn2.weight \t torch.Size([128])\n",
      "50 layer2_a.0.bn2.bias \t torch.Size([128])\n",
      "51 layer2_a.0.bn2.running_mean \t torch.Size([128])\n",
      "52 layer2_a.0.bn2.running_var \t torch.Size([128])\n",
      "53 layer2_a.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "54 layer2_a.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "55 layer2_a.0.downsample.1.weight \t torch.Size([128])\n",
      "56 layer2_a.0.downsample.1.bias \t torch.Size([128])\n",
      "57 layer2_a.0.downsample.1.running_mean \t torch.Size([128])\n",
      "58 layer2_a.0.downsample.1.running_var \t torch.Size([128])\n",
      "59 layer2_a.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "60 layer2_a.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "61 layer2_a.1.bn1.weight \t torch.Size([128])\n",
      "62 layer2_a.1.bn1.bias \t torch.Size([128])\n",
      "63 layer2_a.1.bn1.running_mean \t torch.Size([128])\n",
      "64 layer2_a.1.bn1.running_var \t torch.Size([128])\n",
      "65 layer2_a.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "66 layer2_a.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "67 layer2_a.1.bn2.weight \t torch.Size([128])\n",
      "68 layer2_a.1.bn2.bias \t torch.Size([128])\n",
      "69 layer2_a.1.bn2.running_mean \t torch.Size([128])\n",
      "70 layer2_a.1.bn2.running_var \t torch.Size([128])\n",
      "71 layer2_a.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "72 layer2_a.2.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "73 layer2_a.2.bn1.weight \t torch.Size([128])\n",
      "74 layer2_a.2.bn1.bias \t torch.Size([128])\n",
      "75 layer2_a.2.bn1.running_mean \t torch.Size([128])\n",
      "76 layer2_a.2.bn1.running_var \t torch.Size([128])\n",
      "77 layer2_a.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "78 layer2_a.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "79 layer2_a.2.bn2.weight \t torch.Size([128])\n",
      "80 layer2_a.2.bn2.bias \t torch.Size([128])\n",
      "81 layer2_a.2.bn2.running_mean \t torch.Size([128])\n",
      "82 layer2_a.2.bn2.running_var \t torch.Size([128])\n",
      "83 layer2_a.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "84 layer2_a.3.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "85 layer2_a.3.bn1.weight \t torch.Size([128])\n",
      "86 layer2_a.3.bn1.bias \t torch.Size([128])\n",
      "87 layer2_a.3.bn1.running_mean \t torch.Size([128])\n",
      "88 layer2_a.3.bn1.running_var \t torch.Size([128])\n",
      "89 layer2_a.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "90 layer2_a.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "91 layer2_a.3.bn2.weight \t torch.Size([128])\n",
      "92 layer2_a.3.bn2.bias \t torch.Size([128])\n",
      "93 layer2_a.3.bn2.running_mean \t torch.Size([128])\n",
      "94 layer2_a.3.bn2.running_var \t torch.Size([128])\n",
      "95 layer2_a.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "96 layer3_a.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "97 layer3_a.0.bn1.weight \t torch.Size([256])\n",
      "98 layer3_a.0.bn1.bias \t torch.Size([256])\n",
      "99 layer3_a.0.bn1.running_mean \t torch.Size([256])\n",
      "100 layer3_a.0.bn1.running_var \t torch.Size([256])\n",
      "101 layer3_a.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "102 layer3_a.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "103 layer3_a.0.bn2.weight \t torch.Size([256])\n",
      "104 layer3_a.0.bn2.bias \t torch.Size([256])\n",
      "105 layer3_a.0.bn2.running_mean \t torch.Size([256])\n",
      "106 layer3_a.0.bn2.running_var \t torch.Size([256])\n",
      "107 layer3_a.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "108 layer3_a.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "109 layer3_a.0.downsample.1.weight \t torch.Size([256])\n",
      "110 layer3_a.0.downsample.1.bias \t torch.Size([256])\n",
      "111 layer3_a.0.downsample.1.running_mean \t torch.Size([256])\n",
      "112 layer3_a.0.downsample.1.running_var \t torch.Size([256])\n",
      "113 layer3_a.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "114 layer3_a.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "115 layer3_a.1.bn1.weight \t torch.Size([256])\n",
      "116 layer3_a.1.bn1.bias \t torch.Size([256])\n",
      "117 layer3_a.1.bn1.running_mean \t torch.Size([256])\n",
      "118 layer3_a.1.bn1.running_var \t torch.Size([256])\n",
      "119 layer3_a.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "120 layer3_a.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "121 layer3_a.1.bn2.weight \t torch.Size([256])\n",
      "122 layer3_a.1.bn2.bias \t torch.Size([256])\n",
      "123 layer3_a.1.bn2.running_mean \t torch.Size([256])\n",
      "124 layer3_a.1.bn2.running_var \t torch.Size([256])\n",
      "125 layer3_a.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "126 layer3_a.2.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "127 layer3_a.2.bn1.weight \t torch.Size([256])\n",
      "128 layer3_a.2.bn1.bias \t torch.Size([256])\n",
      "129 layer3_a.2.bn1.running_mean \t torch.Size([256])\n",
      "130 layer3_a.2.bn1.running_var \t torch.Size([256])\n",
      "131 layer3_a.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "132 layer3_a.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "133 layer3_a.2.bn2.weight \t torch.Size([256])\n",
      "134 layer3_a.2.bn2.bias \t torch.Size([256])\n",
      "135 layer3_a.2.bn2.running_mean \t torch.Size([256])\n",
      "136 layer3_a.2.bn2.running_var \t torch.Size([256])\n",
      "137 layer3_a.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "138 layer3_a.3.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "139 layer3_a.3.bn1.weight \t torch.Size([256])\n",
      "140 layer3_a.3.bn1.bias \t torch.Size([256])\n",
      "141 layer3_a.3.bn1.running_mean \t torch.Size([256])\n",
      "142 layer3_a.3.bn1.running_var \t torch.Size([256])\n",
      "143 layer3_a.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "144 layer3_a.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "145 layer3_a.3.bn2.weight \t torch.Size([256])\n",
      "146 layer3_a.3.bn2.bias \t torch.Size([256])\n",
      "147 layer3_a.3.bn2.running_mean \t torch.Size([256])\n",
      "148 layer3_a.3.bn2.running_var \t torch.Size([256])\n",
      "149 layer3_a.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "150 layer3_a.4.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "151 layer3_a.4.bn1.weight \t torch.Size([256])\n",
      "152 layer3_a.4.bn1.bias \t torch.Size([256])\n",
      "153 layer3_a.4.bn1.running_mean \t torch.Size([256])\n",
      "154 layer3_a.4.bn1.running_var \t torch.Size([256])\n",
      "155 layer3_a.4.bn1.num_batches_tracked \t torch.Size([])\n",
      "156 layer3_a.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "157 layer3_a.4.bn2.weight \t torch.Size([256])\n",
      "158 layer3_a.4.bn2.bias \t torch.Size([256])\n",
      "159 layer3_a.4.bn2.running_mean \t torch.Size([256])\n",
      "160 layer3_a.4.bn2.running_var \t torch.Size([256])\n",
      "161 layer3_a.4.bn2.num_batches_tracked \t torch.Size([])\n",
      "162 layer3_a.5.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "163 layer3_a.5.bn1.weight \t torch.Size([256])\n",
      "164 layer3_a.5.bn1.bias \t torch.Size([256])\n",
      "165 layer3_a.5.bn1.running_mean \t torch.Size([256])\n",
      "166 layer3_a.5.bn1.running_var \t torch.Size([256])\n",
      "167 layer3_a.5.bn1.num_batches_tracked \t torch.Size([])\n",
      "168 layer3_a.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "169 layer3_a.5.bn2.weight \t torch.Size([256])\n",
      "170 layer3_a.5.bn2.bias \t torch.Size([256])\n",
      "171 layer3_a.5.bn2.running_mean \t torch.Size([256])\n",
      "172 layer3_a.5.bn2.running_var \t torch.Size([256])\n",
      "173 layer3_a.5.bn2.num_batches_tracked \t torch.Size([])\n",
      "174 layer4_a.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "175 layer4_a.0.bn1.weight \t torch.Size([512])\n",
      "176 layer4_a.0.bn1.bias \t torch.Size([512])\n",
      "177 layer4_a.0.bn1.running_mean \t torch.Size([512])\n",
      "178 layer4_a.0.bn1.running_var \t torch.Size([512])\n",
      "179 layer4_a.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "180 layer4_a.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "181 layer4_a.0.bn2.weight \t torch.Size([512])\n",
      "182 layer4_a.0.bn2.bias \t torch.Size([512])\n",
      "183 layer4_a.0.bn2.running_mean \t torch.Size([512])\n",
      "184 layer4_a.0.bn2.running_var \t torch.Size([512])\n",
      "185 layer4_a.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "186 layer4_a.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "187 layer4_a.0.downsample.1.weight \t torch.Size([512])\n",
      "188 layer4_a.0.downsample.1.bias \t torch.Size([512])\n",
      "189 layer4_a.0.downsample.1.running_mean \t torch.Size([512])\n",
      "190 layer4_a.0.downsample.1.running_var \t torch.Size([512])\n",
      "191 layer4_a.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "192 layer4_a.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "193 layer4_a.1.bn1.weight \t torch.Size([512])\n",
      "194 layer4_a.1.bn1.bias \t torch.Size([512])\n",
      "195 layer4_a.1.bn1.running_mean \t torch.Size([512])\n",
      "196 layer4_a.1.bn1.running_var \t torch.Size([512])\n",
      "197 layer4_a.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "198 layer4_a.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "199 layer4_a.1.bn2.weight \t torch.Size([512])\n",
      "200 layer4_a.1.bn2.bias \t torch.Size([512])\n",
      "201 layer4_a.1.bn2.running_mean \t torch.Size([512])\n",
      "202 layer4_a.1.bn2.running_var \t torch.Size([512])\n",
      "203 layer4_a.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "204 layer4_a.2.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "205 layer4_a.2.bn1.weight \t torch.Size([512])\n",
      "206 layer4_a.2.bn1.bias \t torch.Size([512])\n",
      "207 layer4_a.2.bn1.running_mean \t torch.Size([512])\n",
      "208 layer4_a.2.bn1.running_var \t torch.Size([512])\n",
      "209 layer4_a.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "210 layer4_a.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "211 layer4_a.2.bn2.weight \t torch.Size([512])\n",
      "212 layer4_a.2.bn2.bias \t torch.Size([512])\n",
      "213 layer4_a.2.bn2.running_mean \t torch.Size([512])\n",
      "214 layer4_a.2.bn2.running_var \t torch.Size([512])\n",
      "215 layer4_a.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "216 conv1_b.weight \t torch.Size([64, 20, 7, 7])\n",
      "217 bn1_b.weight \t torch.Size([64])\n",
      "218 bn1_b.bias \t torch.Size([64])\n",
      "219 bn1_b.running_mean \t torch.Size([64])\n",
      "220 bn1_b.running_var \t torch.Size([64])\n",
      "221 bn1_b.num_batches_tracked \t torch.Size([])\n",
      "222 layer1_b.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "223 layer1_b.0.bn1.weight \t torch.Size([64])\n",
      "224 layer1_b.0.bn1.bias \t torch.Size([64])\n",
      "225 layer1_b.0.bn1.running_mean \t torch.Size([64])\n",
      "226 layer1_b.0.bn1.running_var \t torch.Size([64])\n",
      "227 layer1_b.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "228 layer1_b.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "229 layer1_b.0.bn2.weight \t torch.Size([64])\n",
      "230 layer1_b.0.bn2.bias \t torch.Size([64])\n",
      "231 layer1_b.0.bn2.running_mean \t torch.Size([64])\n",
      "232 layer1_b.0.bn2.running_var \t torch.Size([64])\n",
      "233 layer1_b.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "234 layer1_b.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "235 layer1_b.1.bn1.weight \t torch.Size([64])\n",
      "236 layer1_b.1.bn1.bias \t torch.Size([64])\n",
      "237 layer1_b.1.bn1.running_mean \t torch.Size([64])\n",
      "238 layer1_b.1.bn1.running_var \t torch.Size([64])\n",
      "239 layer1_b.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "240 layer1_b.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "241 layer1_b.1.bn2.weight \t torch.Size([64])\n",
      "242 layer1_b.1.bn2.bias \t torch.Size([64])\n",
      "243 layer1_b.1.bn2.running_mean \t torch.Size([64])\n",
      "244 layer1_b.1.bn2.running_var \t torch.Size([64])\n",
      "245 layer1_b.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "246 layer1_b.2.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "247 layer1_b.2.bn1.weight \t torch.Size([64])\n",
      "248 layer1_b.2.bn1.bias \t torch.Size([64])\n",
      "249 layer1_b.2.bn1.running_mean \t torch.Size([64])\n",
      "250 layer1_b.2.bn1.running_var \t torch.Size([64])\n",
      "251 layer1_b.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "252 layer1_b.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "253 layer1_b.2.bn2.weight \t torch.Size([64])\n",
      "254 layer1_b.2.bn2.bias \t torch.Size([64])\n",
      "255 layer1_b.2.bn2.running_mean \t torch.Size([64])\n",
      "256 layer1_b.2.bn2.running_var \t torch.Size([64])\n",
      "257 layer1_b.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "258 layer2_b.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "259 layer2_b.0.bn1.weight \t torch.Size([128])\n",
      "260 layer2_b.0.bn1.bias \t torch.Size([128])\n",
      "261 layer2_b.0.bn1.running_mean \t torch.Size([128])\n",
      "262 layer2_b.0.bn1.running_var \t torch.Size([128])\n",
      "263 layer2_b.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "264 layer2_b.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "265 layer2_b.0.bn2.weight \t torch.Size([128])\n",
      "266 layer2_b.0.bn2.bias \t torch.Size([128])\n",
      "267 layer2_b.0.bn2.running_mean \t torch.Size([128])\n",
      "268 layer2_b.0.bn2.running_var \t torch.Size([128])\n",
      "269 layer2_b.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "270 layer2_b.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "271 layer2_b.0.downsample.1.weight \t torch.Size([128])\n",
      "272 layer2_b.0.downsample.1.bias \t torch.Size([128])\n",
      "273 layer2_b.0.downsample.1.running_mean \t torch.Size([128])\n",
      "274 layer2_b.0.downsample.1.running_var \t torch.Size([128])\n",
      "275 layer2_b.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "276 layer2_b.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "277 layer2_b.1.bn1.weight \t torch.Size([128])\n",
      "278 layer2_b.1.bn1.bias \t torch.Size([128])\n",
      "279 layer2_b.1.bn1.running_mean \t torch.Size([128])\n",
      "280 layer2_b.1.bn1.running_var \t torch.Size([128])\n",
      "281 layer2_b.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "282 layer2_b.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "283 layer2_b.1.bn2.weight \t torch.Size([128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 layer2_b.1.bn2.bias \t torch.Size([128])\n",
      "285 layer2_b.1.bn2.running_mean \t torch.Size([128])\n",
      "286 layer2_b.1.bn2.running_var \t torch.Size([128])\n",
      "287 layer2_b.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "288 layer2_b.2.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "289 layer2_b.2.bn1.weight \t torch.Size([128])\n",
      "290 layer2_b.2.bn1.bias \t torch.Size([128])\n",
      "291 layer2_b.2.bn1.running_mean \t torch.Size([128])\n",
      "292 layer2_b.2.bn1.running_var \t torch.Size([128])\n",
      "293 layer2_b.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "294 layer2_b.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "295 layer2_b.2.bn2.weight \t torch.Size([128])\n",
      "296 layer2_b.2.bn2.bias \t torch.Size([128])\n",
      "297 layer2_b.2.bn2.running_mean \t torch.Size([128])\n",
      "298 layer2_b.2.bn2.running_var \t torch.Size([128])\n",
      "299 layer2_b.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "300 layer2_b.3.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "301 layer2_b.3.bn1.weight \t torch.Size([128])\n",
      "302 layer2_b.3.bn1.bias \t torch.Size([128])\n",
      "303 layer2_b.3.bn1.running_mean \t torch.Size([128])\n",
      "304 layer2_b.3.bn1.running_var \t torch.Size([128])\n",
      "305 layer2_b.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "306 layer2_b.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "307 layer2_b.3.bn2.weight \t torch.Size([128])\n",
      "308 layer2_b.3.bn2.bias \t torch.Size([128])\n",
      "309 layer2_b.3.bn2.running_mean \t torch.Size([128])\n",
      "310 layer2_b.3.bn2.running_var \t torch.Size([128])\n",
      "311 layer2_b.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "312 layer3_b.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "313 layer3_b.0.bn1.weight \t torch.Size([256])\n",
      "314 layer3_b.0.bn1.bias \t torch.Size([256])\n",
      "315 layer3_b.0.bn1.running_mean \t torch.Size([256])\n",
      "316 layer3_b.0.bn1.running_var \t torch.Size([256])\n",
      "317 layer3_b.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "318 layer3_b.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "319 layer3_b.0.bn2.weight \t torch.Size([256])\n",
      "320 layer3_b.0.bn2.bias \t torch.Size([256])\n",
      "321 layer3_b.0.bn2.running_mean \t torch.Size([256])\n",
      "322 layer3_b.0.bn2.running_var \t torch.Size([256])\n",
      "323 layer3_b.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "324 layer3_b.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "325 layer3_b.0.downsample.1.weight \t torch.Size([256])\n",
      "326 layer3_b.0.downsample.1.bias \t torch.Size([256])\n",
      "327 layer3_b.0.downsample.1.running_mean \t torch.Size([256])\n",
      "328 layer3_b.0.downsample.1.running_var \t torch.Size([256])\n",
      "329 layer3_b.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "330 layer3_b.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "331 layer3_b.1.bn1.weight \t torch.Size([256])\n",
      "332 layer3_b.1.bn1.bias \t torch.Size([256])\n",
      "333 layer3_b.1.bn1.running_mean \t torch.Size([256])\n",
      "334 layer3_b.1.bn1.running_var \t torch.Size([256])\n",
      "335 layer3_b.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "336 layer3_b.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "337 layer3_b.1.bn2.weight \t torch.Size([256])\n",
      "338 layer3_b.1.bn2.bias \t torch.Size([256])\n",
      "339 layer3_b.1.bn2.running_mean \t torch.Size([256])\n",
      "340 layer3_b.1.bn2.running_var \t torch.Size([256])\n",
      "341 layer3_b.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "342 layer3_b.2.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "343 layer3_b.2.bn1.weight \t torch.Size([256])\n",
      "344 layer3_b.2.bn1.bias \t torch.Size([256])\n",
      "345 layer3_b.2.bn1.running_mean \t torch.Size([256])\n",
      "346 layer3_b.2.bn1.running_var \t torch.Size([256])\n",
      "347 layer3_b.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "348 layer3_b.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "349 layer3_b.2.bn2.weight \t torch.Size([256])\n",
      "350 layer3_b.2.bn2.bias \t torch.Size([256])\n",
      "351 layer3_b.2.bn2.running_mean \t torch.Size([256])\n",
      "352 layer3_b.2.bn2.running_var \t torch.Size([256])\n",
      "353 layer3_b.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "354 layer3_b.3.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "355 layer3_b.3.bn1.weight \t torch.Size([256])\n",
      "356 layer3_b.3.bn1.bias \t torch.Size([256])\n",
      "357 layer3_b.3.bn1.running_mean \t torch.Size([256])\n",
      "358 layer3_b.3.bn1.running_var \t torch.Size([256])\n",
      "359 layer3_b.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "360 layer3_b.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "361 layer3_b.3.bn2.weight \t torch.Size([256])\n",
      "362 layer3_b.3.bn2.bias \t torch.Size([256])\n",
      "363 layer3_b.3.bn2.running_mean \t torch.Size([256])\n",
      "364 layer3_b.3.bn2.running_var \t torch.Size([256])\n",
      "365 layer3_b.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "366 layer3_b.4.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "367 layer3_b.4.bn1.weight \t torch.Size([256])\n",
      "368 layer3_b.4.bn1.bias \t torch.Size([256])\n",
      "369 layer3_b.4.bn1.running_mean \t torch.Size([256])\n",
      "370 layer3_b.4.bn1.running_var \t torch.Size([256])\n",
      "371 layer3_b.4.bn1.num_batches_tracked \t torch.Size([])\n",
      "372 layer3_b.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "373 layer3_b.4.bn2.weight \t torch.Size([256])\n",
      "374 layer3_b.4.bn2.bias \t torch.Size([256])\n",
      "375 layer3_b.4.bn2.running_mean \t torch.Size([256])\n",
      "376 layer3_b.4.bn2.running_var \t torch.Size([256])\n",
      "377 layer3_b.4.bn2.num_batches_tracked \t torch.Size([])\n",
      "378 layer3_b.5.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "379 layer3_b.5.bn1.weight \t torch.Size([256])\n",
      "380 layer3_b.5.bn1.bias \t torch.Size([256])\n",
      "381 layer3_b.5.bn1.running_mean \t torch.Size([256])\n",
      "382 layer3_b.5.bn1.running_var \t torch.Size([256])\n",
      "383 layer3_b.5.bn1.num_batches_tracked \t torch.Size([])\n",
      "384 layer3_b.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "385 layer3_b.5.bn2.weight \t torch.Size([256])\n",
      "386 layer3_b.5.bn2.bias \t torch.Size([256])\n",
      "387 layer3_b.5.bn2.running_mean \t torch.Size([256])\n",
      "388 layer3_b.5.bn2.running_var \t torch.Size([256])\n",
      "389 layer3_b.5.bn2.num_batches_tracked \t torch.Size([])\n",
      "390 layer4_b.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "391 layer4_b.0.bn1.weight \t torch.Size([512])\n",
      "392 layer4_b.0.bn1.bias \t torch.Size([512])\n",
      "393 layer4_b.0.bn1.running_mean \t torch.Size([512])\n",
      "394 layer4_b.0.bn1.running_var \t torch.Size([512])\n",
      "395 layer4_b.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "396 layer4_b.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "397 layer4_b.0.bn2.weight \t torch.Size([512])\n",
      "398 layer4_b.0.bn2.bias \t torch.Size([512])\n",
      "399 layer4_b.0.bn2.running_mean \t torch.Size([512])\n",
      "400 layer4_b.0.bn2.running_var \t torch.Size([512])\n",
      "401 layer4_b.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "402 layer4_b.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "403 layer4_b.0.downsample.1.weight \t torch.Size([512])\n",
      "404 layer4_b.0.downsample.1.bias \t torch.Size([512])\n",
      "405 layer4_b.0.downsample.1.running_mean \t torch.Size([512])\n",
      "406 layer4_b.0.downsample.1.running_var \t torch.Size([512])\n",
      "407 layer4_b.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "408 layer4_b.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "409 layer4_b.1.bn1.weight \t torch.Size([512])\n",
      "410 layer4_b.1.bn1.bias \t torch.Size([512])\n",
      "411 layer4_b.1.bn1.running_mean \t torch.Size([512])\n",
      "412 layer4_b.1.bn1.running_var \t torch.Size([512])\n",
      "413 layer4_b.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "414 layer4_b.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "415 layer4_b.1.bn2.weight \t torch.Size([512])\n",
      "416 layer4_b.1.bn2.bias \t torch.Size([512])\n",
      "417 layer4_b.1.bn2.running_mean \t torch.Size([512])\n",
      "418 layer4_b.1.bn2.running_var \t torch.Size([512])\n",
      "419 layer4_b.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "420 layer4_b.2.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "421 layer4_b.2.bn1.weight \t torch.Size([512])\n",
      "422 layer4_b.2.bn1.bias \t torch.Size([512])\n",
      "423 layer4_b.2.bn1.running_mean \t torch.Size([512])\n",
      "424 layer4_b.2.bn1.running_var \t torch.Size([512])\n",
      "425 layer4_b.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "426 layer4_b.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "427 layer4_b.2.bn2.weight \t torch.Size([512])\n",
      "428 layer4_b.2.bn2.bias \t torch.Size([512])\n",
      "429 layer4_b.2.bn2.running_mean \t torch.Size([512])\n",
      "430 layer4_b.2.bn2.running_var \t torch.Size([512])\n",
      "431 layer4_b.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "432 bn_f1.weight \t torch.Size([1024])\n",
      "433 bn_f1.bias \t torch.Size([1024])\n",
      "434 bn_f1.running_mean \t torch.Size([1024])\n",
      "435 bn_f1.running_var \t torch.Size([1024])\n",
      "436 bn_f1.num_batches_tracked \t torch.Size([])\n",
      "437 fc_action.weight \t torch.Size([2, 1024])\n",
      "438 fc_action.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "model = twostream_resnet34(True, rgb_channel=3, flow_channel=20, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_a.weight False\n",
      "bn1_a.weight False\n",
      "bn1_a.bias False\n",
      "layer1_a.0.conv1.weight False\n",
      "layer1_a.0.bn1.weight False\n",
      "layer1_a.0.bn1.bias False\n",
      "layer1_a.0.conv2.weight False\n",
      "layer1_a.0.bn2.weight False\n",
      "layer1_a.0.bn2.bias False\n",
      "layer1_a.1.conv1.weight False\n",
      "layer1_a.1.bn1.weight False\n",
      "layer1_a.1.bn1.bias False\n",
      "layer1_a.1.conv2.weight False\n",
      "layer1_a.1.bn2.weight False\n",
      "layer1_a.1.bn2.bias False\n",
      "layer1_a.2.conv1.weight False\n",
      "layer1_a.2.bn1.weight False\n",
      "layer1_a.2.bn1.bias False\n",
      "layer1_a.2.conv2.weight False\n",
      "layer1_a.2.bn2.weight False\n",
      "layer1_a.2.bn2.bias False\n",
      "layer2_a.0.conv1.weight False\n",
      "layer2_a.0.bn1.weight False\n",
      "layer2_a.0.bn1.bias False\n",
      "layer2_a.0.conv2.weight False\n",
      "layer2_a.0.bn2.weight False\n",
      "layer2_a.0.bn2.bias False\n",
      "layer2_a.0.downsample.0.weight False\n",
      "layer2_a.0.downsample.1.weight False\n",
      "layer2_a.0.downsample.1.bias False\n",
      "layer2_a.1.conv1.weight False\n",
      "layer2_a.1.bn1.weight False\n",
      "layer2_a.1.bn1.bias False\n",
      "layer2_a.1.conv2.weight False\n",
      "layer2_a.1.bn2.weight False\n",
      "layer2_a.1.bn2.bias False\n",
      "layer2_a.2.conv1.weight False\n",
      "layer2_a.2.bn1.weight False\n",
      "layer2_a.2.bn1.bias False\n",
      "layer2_a.2.conv2.weight False\n",
      "layer2_a.2.bn2.weight False\n",
      "layer2_a.2.bn2.bias False\n",
      "layer2_a.3.conv1.weight False\n",
      "layer2_a.3.bn1.weight False\n",
      "layer2_a.3.bn1.bias False\n",
      "layer2_a.3.conv2.weight False\n",
      "layer2_a.3.bn2.weight False\n",
      "layer2_a.3.bn2.bias False\n",
      "layer3_a.0.conv1.weight False\n",
      "layer3_a.0.bn1.weight False\n",
      "layer3_a.0.bn1.bias False\n",
      "layer3_a.0.conv2.weight False\n",
      "layer3_a.0.bn2.weight False\n",
      "layer3_a.0.bn2.bias False\n",
      "layer3_a.0.downsample.0.weight False\n",
      "layer3_a.0.downsample.1.weight False\n",
      "layer3_a.0.downsample.1.bias False\n",
      "layer3_a.1.conv1.weight False\n",
      "layer3_a.1.bn1.weight False\n",
      "layer3_a.1.bn1.bias False\n",
      "layer3_a.1.conv2.weight False\n",
      "layer3_a.1.bn2.weight False\n",
      "layer3_a.1.bn2.bias False\n",
      "layer3_a.2.conv1.weight False\n",
      "layer3_a.2.bn1.weight False\n",
      "layer3_a.2.bn1.bias False\n",
      "layer3_a.2.conv2.weight False\n",
      "layer3_a.2.bn2.weight False\n",
      "layer3_a.2.bn2.bias False\n",
      "layer3_a.3.conv1.weight False\n",
      "layer3_a.3.bn1.weight False\n",
      "layer3_a.3.bn1.bias False\n",
      "layer3_a.3.conv2.weight False\n",
      "layer3_a.3.bn2.weight False\n",
      "layer3_a.3.bn2.bias False\n",
      "layer3_a.4.conv1.weight False\n",
      "layer3_a.4.bn1.weight False\n",
      "layer3_a.4.bn1.bias False\n",
      "layer3_a.4.conv2.weight False\n",
      "layer3_a.4.bn2.weight False\n",
      "layer3_a.4.bn2.bias False\n",
      "layer3_a.5.conv1.weight False\n",
      "layer3_a.5.bn1.weight False\n",
      "layer3_a.5.bn1.bias False\n",
      "layer3_a.5.conv2.weight False\n",
      "layer3_a.5.bn2.weight False\n",
      "layer3_a.5.bn2.bias False\n",
      "layer4_a.0.conv1.weight True\n",
      "layer4_a.0.bn1.weight True\n",
      "layer4_a.0.bn1.bias True\n",
      "layer4_a.0.conv2.weight True\n",
      "layer4_a.0.bn2.weight True\n",
      "layer4_a.0.bn2.bias True\n",
      "layer4_a.0.downsample.0.weight True\n",
      "layer4_a.0.downsample.1.weight True\n",
      "layer4_a.0.downsample.1.bias True\n",
      "layer4_a.1.conv1.weight True\n",
      "layer4_a.1.bn1.weight True\n",
      "layer4_a.1.bn1.bias True\n",
      "layer4_a.1.conv2.weight True\n",
      "layer4_a.1.bn2.weight True\n",
      "layer4_a.1.bn2.bias True\n",
      "layer4_a.2.conv1.weight True\n",
      "layer4_a.2.bn1.weight True\n",
      "layer4_a.2.bn1.bias True\n",
      "layer4_a.2.conv2.weight True\n",
      "layer4_a.2.bn2.weight True\n",
      "layer4_a.2.bn2.bias True\n",
      "conv1_b.weight False\n",
      "bn1_b.weight False\n",
      "bn1_b.bias False\n",
      "layer1_b.0.conv1.weight False\n",
      "layer1_b.0.bn1.weight False\n",
      "layer1_b.0.bn1.bias False\n",
      "layer1_b.0.conv2.weight False\n",
      "layer1_b.0.bn2.weight False\n",
      "layer1_b.0.bn2.bias False\n",
      "layer1_b.1.conv1.weight False\n",
      "layer1_b.1.bn1.weight False\n",
      "layer1_b.1.bn1.bias False\n",
      "layer1_b.1.conv2.weight False\n",
      "layer1_b.1.bn2.weight False\n",
      "layer1_b.1.bn2.bias False\n",
      "layer1_b.2.conv1.weight False\n",
      "layer1_b.2.bn1.weight False\n",
      "layer1_b.2.bn1.bias False\n",
      "layer1_b.2.conv2.weight False\n",
      "layer1_b.2.bn2.weight False\n",
      "layer1_b.2.bn2.bias False\n",
      "layer2_b.0.conv1.weight False\n",
      "layer2_b.0.bn1.weight False\n",
      "layer2_b.0.bn1.bias False\n",
      "layer2_b.0.conv2.weight False\n",
      "layer2_b.0.bn2.weight False\n",
      "layer2_b.0.bn2.bias False\n",
      "layer2_b.0.downsample.0.weight False\n",
      "layer2_b.0.downsample.1.weight False\n",
      "layer2_b.0.downsample.1.bias False\n",
      "layer2_b.1.conv1.weight False\n",
      "layer2_b.1.bn1.weight False\n",
      "layer2_b.1.bn1.bias False\n",
      "layer2_b.1.conv2.weight False\n",
      "layer2_b.1.bn2.weight False\n",
      "layer2_b.1.bn2.bias False\n",
      "layer2_b.2.conv1.weight False\n",
      "layer2_b.2.bn1.weight False\n",
      "layer2_b.2.bn1.bias False\n",
      "layer2_b.2.conv2.weight False\n",
      "layer2_b.2.bn2.weight False\n",
      "layer2_b.2.bn2.bias False\n",
      "layer2_b.3.conv1.weight False\n",
      "layer2_b.3.bn1.weight False\n",
      "layer2_b.3.bn1.bias False\n",
      "layer2_b.3.conv2.weight False\n",
      "layer2_b.3.bn2.weight False\n",
      "layer2_b.3.bn2.bias False\n",
      "layer3_b.0.conv1.weight False\n",
      "layer3_b.0.bn1.weight False\n",
      "layer3_b.0.bn1.bias False\n",
      "layer3_b.0.conv2.weight False\n",
      "layer3_b.0.bn2.weight False\n",
      "layer3_b.0.bn2.bias False\n",
      "layer3_b.0.downsample.0.weight False\n",
      "layer3_b.0.downsample.1.weight False\n",
      "layer3_b.0.downsample.1.bias False\n",
      "layer3_b.1.conv1.weight False\n",
      "layer3_b.1.bn1.weight False\n",
      "layer3_b.1.bn1.bias False\n",
      "layer3_b.1.conv2.weight False\n",
      "layer3_b.1.bn2.weight False\n",
      "layer3_b.1.bn2.bias False\n",
      "layer3_b.2.conv1.weight False\n",
      "layer3_b.2.bn1.weight False\n",
      "layer3_b.2.bn1.bias False\n",
      "layer3_b.2.conv2.weight False\n",
      "layer3_b.2.bn2.weight False\n",
      "layer3_b.2.bn2.bias False\n",
      "layer3_b.3.conv1.weight False\n",
      "layer3_b.3.bn1.weight False\n",
      "layer3_b.3.bn1.bias False\n",
      "layer3_b.3.conv2.weight False\n",
      "layer3_b.3.bn2.weight False\n",
      "layer3_b.3.bn2.bias False\n",
      "layer3_b.4.conv1.weight False\n",
      "layer3_b.4.bn1.weight False\n",
      "layer3_b.4.bn1.bias False\n",
      "layer3_b.4.conv2.weight False\n",
      "layer3_b.4.bn2.weight False\n",
      "layer3_b.4.bn2.bias False\n",
      "layer3_b.5.conv1.weight False\n",
      "layer3_b.5.bn1.weight False\n",
      "layer3_b.5.bn1.bias False\n",
      "layer3_b.5.conv2.weight False\n",
      "layer3_b.5.bn2.weight False\n",
      "layer3_b.5.bn2.bias False\n",
      "layer4_b.0.conv1.weight True\n",
      "layer4_b.0.bn1.weight True\n",
      "layer4_b.0.bn1.bias True\n",
      "layer4_b.0.conv2.weight True\n",
      "layer4_b.0.bn2.weight True\n",
      "layer4_b.0.bn2.bias True\n",
      "layer4_b.0.downsample.0.weight True\n",
      "layer4_b.0.downsample.1.weight True\n",
      "layer4_b.0.downsample.1.bias True\n",
      "layer4_b.1.conv1.weight True\n",
      "layer4_b.1.bn1.weight True\n",
      "layer4_b.1.bn1.bias True\n",
      "layer4_b.1.conv2.weight True\n",
      "layer4_b.1.bn2.weight True\n",
      "layer4_b.1.bn2.bias True\n",
      "layer4_b.2.conv1.weight True\n",
      "layer4_b.2.bn1.weight True\n",
      "layer4_b.2.bn1.bias True\n",
      "layer4_b.2.conv2.weight True\n",
      "layer4_b.2.bn2.weight True\n",
      "layer4_b.2.bn2.bias True\n",
      "bn_f1.weight True\n",
      "bn_f1.bias True\n",
      "fc_action.weight True\n",
      "fc_action.bias True\n"
     ]
    }
   ],
   "source": [
    "def set_freeze_layers(model):\n",
    "    freeze_layers = ['conv1', 'bn1', 'layer1', 'layer2', 'layer3']\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = True \n",
    "        for ll in freeze_layers:\n",
    "            if ll in name.split('.')[0]:\n",
    "                param.requires_grad = False\n",
    "                break    \n",
    "          \n",
    "set_freeze_layers(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_action.bias\n"
     ]
    }
   ],
   "source": [
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
