{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB resnet test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        # self.fc_aux = nn.Linear(512 * block.expansion, 101)\n",
    "        self.dp = nn.Dropout(p=0.8)\n",
    "        self.fc_action = nn.Linear(512 * block.expansion, num_classes)\n",
    "        # self.bn_final = nn.BatchNorm1d(num_classes)\n",
    "        # self.fc2 = nn.Linear(num_classes, num_classes)\n",
    "        # self.fc_final = nn.Linear(num_classes, 101)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dp(x)\n",
    "        x = self.fc_action(x)\n",
    "        # x = self.bn_final(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.fc_final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def rgb_resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def rgb_resnet34(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "#     model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    if pretrained:\n",
    "        pretrained_dict = model_zoo.load_url(model_urls['resnet34'])\n",
    "        print(\"pretrained_dict's state_dict:\", len(pretrained_dict))\n",
    "        for i, param_tensor in enumerate(pretrained_dict):\n",
    "            print(i, param_tensor, \"\\t\", pretrained_dict[param_tensor].size())\n",
    "            \n",
    "    \n",
    "        model_dict = model.state_dict()\n",
    "        print('------'*20)\n",
    "        print(\"model_dict's state_dict:\", len(model_dict))\n",
    "        for i, param_tensor in enumerate(model_dict):\n",
    "            print(i, param_tensor, \"\\t\", model_dict[param_tensor].size())\n",
    "            \n",
    "            \n",
    "        # 1. filter out unnecessary keys\n",
    "        # 沒有在model_dict裡面的key 就拿掉\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        \n",
    "        print('------'*20)\n",
    "        print(\"filter out unnecessary keys pretrained_dict's state_dict:\", len(pretrained_dict))\n",
    "        for i, param_tensor in enumerate(pretrained_dict):\n",
    "            print(i, param_tensor, \"\\t\", pretrained_dict[param_tensor].size())\n",
    "            \n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        # use pretrained_dict value , update model_dict value\n",
    "        model_dict.update(pretrained_dict) \n",
    "        print('------'*20)\n",
    "        print(\"update model_dict's state_dict:\", len(model_dict))\n",
    "        for i, param_tensor in enumerate(model_dict):\n",
    "            print(i, param_tensor, \"\\t\", model_dict[param_tensor].size())\n",
    "                        \n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "        \n",
    "    return model\n",
    "\n",
    "# 50 以後用Bottleneck\n",
    "def rgb_resnet50(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        # model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "        pretrained_dict = model_zoo.load_url(model_urls['resnet50'])\n",
    "\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        # 1. filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict) \n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def rgb_resnet50_aux(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        # model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "        pretrained_dict = model_zoo.load_url(model_urls['resnet50'])\n",
    "\n",
    "        model_dict = model.state_dict()\n",
    "        fc_origin_weight = pretrained_dict[\"fc.weight\"].data.numpy()\n",
    "        fc_origin_bias = pretrained_dict[\"fc.bias\"].data.numpy()\n",
    "\n",
    "        # 1. filter out unnecessary keys\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict) \n",
    "        # print(model_dict)\n",
    "        fc_new_weight = model_dict[\"fc_aux.weight\"].numpy() \n",
    "        fc_new_bias = model_dict[\"fc_aux.bias\"].numpy() \n",
    "\n",
    "        fc_new_weight[:1000, :] = fc_origin_weight\n",
    "        fc_new_bias[:1000] = fc_origin_bias\n",
    "\n",
    "        model_dict[\"fc_aux.weight\"] = torch.from_numpy(fc_new_weight)\n",
    "        model_dict[\"fc_aux.bias\"] = torch.from_numpy(fc_new_bias)\n",
    "\n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def rgb_resnet101(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def rgb_resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        # model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "        pretrained_dict = model_zoo.load_url(model_urls['resnet152'])\n",
    "        model_dict = model.state_dict()\n",
    "\n",
    "        # 1. filter out unnecessary keys\n",
    "        # 沒有在model_dict裡面的key 就拿掉\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        # use pretrained_dict value , update model_dict value\n",
    "        model_dict.update(pretrained_dict) \n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to C:\\Users\\leech/.cache\\torch\\hub\\checkpoints\\resnet34-333f7ec4.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d263c387659043e8a12a35140e92e816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pretrained_dict's state_dict: 182\n",
      "0 conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "1 bn1.running_mean \t torch.Size([64])\n",
      "2 bn1.running_var \t torch.Size([64])\n",
      "3 bn1.weight \t torch.Size([64])\n",
      "4 bn1.bias \t torch.Size([64])\n",
      "5 layer1.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "6 layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "7 layer1.0.bn1.running_var \t torch.Size([64])\n",
      "8 layer1.0.bn1.weight \t torch.Size([64])\n",
      "9 layer1.0.bn1.bias \t torch.Size([64])\n",
      "10 layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "11 layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "12 layer1.0.bn2.running_var \t torch.Size([64])\n",
      "13 layer1.0.bn2.weight \t torch.Size([64])\n",
      "14 layer1.0.bn2.bias \t torch.Size([64])\n",
      "15 layer1.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "16 layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "17 layer1.1.bn1.running_var \t torch.Size([64])\n",
      "18 layer1.1.bn1.weight \t torch.Size([64])\n",
      "19 layer1.1.bn1.bias \t torch.Size([64])\n",
      "20 layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "21 layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "22 layer1.1.bn2.running_var \t torch.Size([64])\n",
      "23 layer1.1.bn2.weight \t torch.Size([64])\n",
      "24 layer1.1.bn2.bias \t torch.Size([64])\n",
      "25 layer1.2.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "26 layer1.2.bn1.running_mean \t torch.Size([64])\n",
      "27 layer1.2.bn1.running_var \t torch.Size([64])\n",
      "28 layer1.2.bn1.weight \t torch.Size([64])\n",
      "29 layer1.2.bn1.bias \t torch.Size([64])\n",
      "30 layer1.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "31 layer1.2.bn2.running_mean \t torch.Size([64])\n",
      "32 layer1.2.bn2.running_var \t torch.Size([64])\n",
      "33 layer1.2.bn2.weight \t torch.Size([64])\n",
      "34 layer1.2.bn2.bias \t torch.Size([64])\n",
      "35 layer2.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "36 layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "37 layer2.0.bn1.running_var \t torch.Size([128])\n",
      "38 layer2.0.bn1.weight \t torch.Size([128])\n",
      "39 layer2.0.bn1.bias \t torch.Size([128])\n",
      "40 layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "41 layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "42 layer2.0.bn2.running_var \t torch.Size([128])\n",
      "43 layer2.0.bn2.weight \t torch.Size([128])\n",
      "44 layer2.0.bn2.bias \t torch.Size([128])\n",
      "45 layer2.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "46 layer2.0.downsample.1.running_mean \t torch.Size([128])\n",
      "47 layer2.0.downsample.1.running_var \t torch.Size([128])\n",
      "48 layer2.0.downsample.1.weight \t torch.Size([128])\n",
      "49 layer2.0.downsample.1.bias \t torch.Size([128])\n",
      "50 layer2.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "51 layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "52 layer2.1.bn1.running_var \t torch.Size([128])\n",
      "53 layer2.1.bn1.weight \t torch.Size([128])\n",
      "54 layer2.1.bn1.bias \t torch.Size([128])\n",
      "55 layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "56 layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "57 layer2.1.bn2.running_var \t torch.Size([128])\n",
      "58 layer2.1.bn2.weight \t torch.Size([128])\n",
      "59 layer2.1.bn2.bias \t torch.Size([128])\n",
      "60 layer2.2.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "61 layer2.2.bn1.running_mean \t torch.Size([128])\n",
      "62 layer2.2.bn1.running_var \t torch.Size([128])\n",
      "63 layer2.2.bn1.weight \t torch.Size([128])\n",
      "64 layer2.2.bn1.bias \t torch.Size([128])\n",
      "65 layer2.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "66 layer2.2.bn2.running_mean \t torch.Size([128])\n",
      "67 layer2.2.bn2.running_var \t torch.Size([128])\n",
      "68 layer2.2.bn2.weight \t torch.Size([128])\n",
      "69 layer2.2.bn2.bias \t torch.Size([128])\n",
      "70 layer2.3.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "71 layer2.3.bn1.running_mean \t torch.Size([128])\n",
      "72 layer2.3.bn1.running_var \t torch.Size([128])\n",
      "73 layer2.3.bn1.weight \t torch.Size([128])\n",
      "74 layer2.3.bn1.bias \t torch.Size([128])\n",
      "75 layer2.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "76 layer2.3.bn2.running_mean \t torch.Size([128])\n",
      "77 layer2.3.bn2.running_var \t torch.Size([128])\n",
      "78 layer2.3.bn2.weight \t torch.Size([128])\n",
      "79 layer2.3.bn2.bias \t torch.Size([128])\n",
      "80 layer3.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "81 layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "82 layer3.0.bn1.running_var \t torch.Size([256])\n",
      "83 layer3.0.bn1.weight \t torch.Size([256])\n",
      "84 layer3.0.bn1.bias \t torch.Size([256])\n",
      "85 layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "86 layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "87 layer3.0.bn2.running_var \t torch.Size([256])\n",
      "88 layer3.0.bn2.weight \t torch.Size([256])\n",
      "89 layer3.0.bn2.bias \t torch.Size([256])\n",
      "90 layer3.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "91 layer3.0.downsample.1.running_mean \t torch.Size([256])\n",
      "92 layer3.0.downsample.1.running_var \t torch.Size([256])\n",
      "93 layer3.0.downsample.1.weight \t torch.Size([256])\n",
      "94 layer3.0.downsample.1.bias \t torch.Size([256])\n",
      "95 layer3.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "96 layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "97 layer3.1.bn1.running_var \t torch.Size([256])\n",
      "98 layer3.1.bn1.weight \t torch.Size([256])\n",
      "99 layer3.1.bn1.bias \t torch.Size([256])\n",
      "100 layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "101 layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "102 layer3.1.bn2.running_var \t torch.Size([256])\n",
      "103 layer3.1.bn2.weight \t torch.Size([256])\n",
      "104 layer3.1.bn2.bias \t torch.Size([256])\n",
      "105 layer3.2.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "106 layer3.2.bn1.running_mean \t torch.Size([256])\n",
      "107 layer3.2.bn1.running_var \t torch.Size([256])\n",
      "108 layer3.2.bn1.weight \t torch.Size([256])\n",
      "109 layer3.2.bn1.bias \t torch.Size([256])\n",
      "110 layer3.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "111 layer3.2.bn2.running_mean \t torch.Size([256])\n",
      "112 layer3.2.bn2.running_var \t torch.Size([256])\n",
      "113 layer3.2.bn2.weight \t torch.Size([256])\n",
      "114 layer3.2.bn2.bias \t torch.Size([256])\n",
      "115 layer3.3.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "116 layer3.3.bn1.running_mean \t torch.Size([256])\n",
      "117 layer3.3.bn1.running_var \t torch.Size([256])\n",
      "118 layer3.3.bn1.weight \t torch.Size([256])\n",
      "119 layer3.3.bn1.bias \t torch.Size([256])\n",
      "120 layer3.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "121 layer3.3.bn2.running_mean \t torch.Size([256])\n",
      "122 layer3.3.bn2.running_var \t torch.Size([256])\n",
      "123 layer3.3.bn2.weight \t torch.Size([256])\n",
      "124 layer3.3.bn2.bias \t torch.Size([256])\n",
      "125 layer3.4.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "126 layer3.4.bn1.running_mean \t torch.Size([256])\n",
      "127 layer3.4.bn1.running_var \t torch.Size([256])\n",
      "128 layer3.4.bn1.weight \t torch.Size([256])\n",
      "129 layer3.4.bn1.bias \t torch.Size([256])\n",
      "130 layer3.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "131 layer3.4.bn2.running_mean \t torch.Size([256])\n",
      "132 layer3.4.bn2.running_var \t torch.Size([256])\n",
      "133 layer3.4.bn2.weight \t torch.Size([256])\n",
      "134 layer3.4.bn2.bias \t torch.Size([256])\n",
      "135 layer3.5.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "136 layer3.5.bn1.running_mean \t torch.Size([256])\n",
      "137 layer3.5.bn1.running_var \t torch.Size([256])\n",
      "138 layer3.5.bn1.weight \t torch.Size([256])\n",
      "139 layer3.5.bn1.bias \t torch.Size([256])\n",
      "140 layer3.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "141 layer3.5.bn2.running_mean \t torch.Size([256])\n",
      "142 layer3.5.bn2.running_var \t torch.Size([256])\n",
      "143 layer3.5.bn2.weight \t torch.Size([256])\n",
      "144 layer3.5.bn2.bias \t torch.Size([256])\n",
      "145 layer4.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "146 layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "147 layer4.0.bn1.running_var \t torch.Size([512])\n",
      "148 layer4.0.bn1.weight \t torch.Size([512])\n",
      "149 layer4.0.bn1.bias \t torch.Size([512])\n",
      "150 layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "151 layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "152 layer4.0.bn2.running_var \t torch.Size([512])\n",
      "153 layer4.0.bn2.weight \t torch.Size([512])\n",
      "154 layer4.0.bn2.bias \t torch.Size([512])\n",
      "155 layer4.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "156 layer4.0.downsample.1.running_mean \t torch.Size([512])\n",
      "157 layer4.0.downsample.1.running_var \t torch.Size([512])\n",
      "158 layer4.0.downsample.1.weight \t torch.Size([512])\n",
      "159 layer4.0.downsample.1.bias \t torch.Size([512])\n",
      "160 layer4.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "161 layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "162 layer4.1.bn1.running_var \t torch.Size([512])\n",
      "163 layer4.1.bn1.weight \t torch.Size([512])\n",
      "164 layer4.1.bn1.bias \t torch.Size([512])\n",
      "165 layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "166 layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "167 layer4.1.bn2.running_var \t torch.Size([512])\n",
      "168 layer4.1.bn2.weight \t torch.Size([512])\n",
      "169 layer4.1.bn2.bias \t torch.Size([512])\n",
      "170 layer4.2.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "171 layer4.2.bn1.running_mean \t torch.Size([512])\n",
      "172 layer4.2.bn1.running_var \t torch.Size([512])\n",
      "173 layer4.2.bn1.weight \t torch.Size([512])\n",
      "174 layer4.2.bn1.bias \t torch.Size([512])\n",
      "175 layer4.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "176 layer4.2.bn2.running_mean \t torch.Size([512])\n",
      "177 layer4.2.bn2.running_var \t torch.Size([512])\n",
      "178 layer4.2.bn2.weight \t torch.Size([512])\n",
      "179 layer4.2.bn2.bias \t torch.Size([512])\n",
      "180 fc.weight \t torch.Size([1000, 512])\n",
      "181 fc.bias \t torch.Size([1000])\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "model_dict's state_dict: 218\n",
      "0 conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "1 bn1.weight \t torch.Size([64])\n",
      "2 bn1.bias \t torch.Size([64])\n",
      "3 bn1.running_mean \t torch.Size([64])\n",
      "4 bn1.running_var \t torch.Size([64])\n",
      "5 bn1.num_batches_tracked \t torch.Size([])\n",
      "6 layer1.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "7 layer1.0.bn1.weight \t torch.Size([64])\n",
      "8 layer1.0.bn1.bias \t torch.Size([64])\n",
      "9 layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "10 layer1.0.bn1.running_var \t torch.Size([64])\n",
      "11 layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "12 layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "13 layer1.0.bn2.weight \t torch.Size([64])\n",
      "14 layer1.0.bn2.bias \t torch.Size([64])\n",
      "15 layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "16 layer1.0.bn2.running_var \t torch.Size([64])\n",
      "17 layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "18 layer1.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "19 layer1.1.bn1.weight \t torch.Size([64])\n",
      "20 layer1.1.bn1.bias \t torch.Size([64])\n",
      "21 layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "22 layer1.1.bn1.running_var \t torch.Size([64])\n",
      "23 layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "24 layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "25 layer1.1.bn2.weight \t torch.Size([64])\n",
      "26 layer1.1.bn2.bias \t torch.Size([64])\n",
      "27 layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "28 layer1.1.bn2.running_var \t torch.Size([64])\n",
      "29 layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "30 layer1.2.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "31 layer1.2.bn1.weight \t torch.Size([64])\n",
      "32 layer1.2.bn1.bias \t torch.Size([64])\n",
      "33 layer1.2.bn1.running_mean \t torch.Size([64])\n",
      "34 layer1.2.bn1.running_var \t torch.Size([64])\n",
      "35 layer1.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "36 layer1.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "37 layer1.2.bn2.weight \t torch.Size([64])\n",
      "38 layer1.2.bn2.bias \t torch.Size([64])\n",
      "39 layer1.2.bn2.running_mean \t torch.Size([64])\n",
      "40 layer1.2.bn2.running_var \t torch.Size([64])\n",
      "41 layer1.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "42 layer2.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "43 layer2.0.bn1.weight \t torch.Size([128])\n",
      "44 layer2.0.bn1.bias \t torch.Size([128])\n",
      "45 layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "46 layer2.0.bn1.running_var \t torch.Size([128])\n",
      "47 layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "48 layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "49 layer2.0.bn2.weight \t torch.Size([128])\n",
      "50 layer2.0.bn2.bias \t torch.Size([128])\n",
      "51 layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "52 layer2.0.bn2.running_var \t torch.Size([128])\n",
      "53 layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "54 layer2.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "55 layer2.0.downsample.1.weight \t torch.Size([128])\n",
      "56 layer2.0.downsample.1.bias \t torch.Size([128])\n",
      "57 layer2.0.downsample.1.running_mean \t torch.Size([128])\n",
      "58 layer2.0.downsample.1.running_var \t torch.Size([128])\n",
      "59 layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "60 layer2.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "61 layer2.1.bn1.weight \t torch.Size([128])\n",
      "62 layer2.1.bn1.bias \t torch.Size([128])\n",
      "63 layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "64 layer2.1.bn1.running_var \t torch.Size([128])\n",
      "65 layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "66 layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "67 layer2.1.bn2.weight \t torch.Size([128])\n",
      "68 layer2.1.bn2.bias \t torch.Size([128])\n",
      "69 layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "70 layer2.1.bn2.running_var \t torch.Size([128])\n",
      "71 layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "72 layer2.2.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "73 layer2.2.bn1.weight \t torch.Size([128])\n",
      "74 layer2.2.bn1.bias \t torch.Size([128])\n",
      "75 layer2.2.bn1.running_mean \t torch.Size([128])\n",
      "76 layer2.2.bn1.running_var \t torch.Size([128])\n",
      "77 layer2.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "78 layer2.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "79 layer2.2.bn2.weight \t torch.Size([128])\n",
      "80 layer2.2.bn2.bias \t torch.Size([128])\n",
      "81 layer2.2.bn2.running_mean \t torch.Size([128])\n",
      "82 layer2.2.bn2.running_var \t torch.Size([128])\n",
      "83 layer2.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "84 layer2.3.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "85 layer2.3.bn1.weight \t torch.Size([128])\n",
      "86 layer2.3.bn1.bias \t torch.Size([128])\n",
      "87 layer2.3.bn1.running_mean \t torch.Size([128])\n",
      "88 layer2.3.bn1.running_var \t torch.Size([128])\n",
      "89 layer2.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "90 layer2.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "91 layer2.3.bn2.weight \t torch.Size([128])\n",
      "92 layer2.3.bn2.bias \t torch.Size([128])\n",
      "93 layer2.3.bn2.running_mean \t torch.Size([128])\n",
      "94 layer2.3.bn2.running_var \t torch.Size([128])\n",
      "95 layer2.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "96 layer3.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "97 layer3.0.bn1.weight \t torch.Size([256])\n",
      "98 layer3.0.bn1.bias \t torch.Size([256])\n",
      "99 layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "100 layer3.0.bn1.running_var \t torch.Size([256])\n",
      "101 layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "102 layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "103 layer3.0.bn2.weight \t torch.Size([256])\n",
      "104 layer3.0.bn2.bias \t torch.Size([256])\n",
      "105 layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "106 layer3.0.bn2.running_var \t torch.Size([256])\n",
      "107 layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "108 layer3.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "109 layer3.0.downsample.1.weight \t torch.Size([256])\n",
      "110 layer3.0.downsample.1.bias \t torch.Size([256])\n",
      "111 layer3.0.downsample.1.running_mean \t torch.Size([256])\n",
      "112 layer3.0.downsample.1.running_var \t torch.Size([256])\n",
      "113 layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "114 layer3.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "115 layer3.1.bn1.weight \t torch.Size([256])\n",
      "116 layer3.1.bn1.bias \t torch.Size([256])\n",
      "117 layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "118 layer3.1.bn1.running_var \t torch.Size([256])\n",
      "119 layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "120 layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "121 layer3.1.bn2.weight \t torch.Size([256])\n",
      "122 layer3.1.bn2.bias \t torch.Size([256])\n",
      "123 layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "124 layer3.1.bn2.running_var \t torch.Size([256])\n",
      "125 layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "126 layer3.2.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "127 layer3.2.bn1.weight \t torch.Size([256])\n",
      "128 layer3.2.bn1.bias \t torch.Size([256])\n",
      "129 layer3.2.bn1.running_mean \t torch.Size([256])\n",
      "130 layer3.2.bn1.running_var \t torch.Size([256])\n",
      "131 layer3.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "132 layer3.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "133 layer3.2.bn2.weight \t torch.Size([256])\n",
      "134 layer3.2.bn2.bias \t torch.Size([256])\n",
      "135 layer3.2.bn2.running_mean \t torch.Size([256])\n",
      "136 layer3.2.bn2.running_var \t torch.Size([256])\n",
      "137 layer3.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "138 layer3.3.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "139 layer3.3.bn1.weight \t torch.Size([256])\n",
      "140 layer3.3.bn1.bias \t torch.Size([256])\n",
      "141 layer3.3.bn1.running_mean \t torch.Size([256])\n",
      "142 layer3.3.bn1.running_var \t torch.Size([256])\n",
      "143 layer3.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "144 layer3.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "145 layer3.3.bn2.weight \t torch.Size([256])\n",
      "146 layer3.3.bn2.bias \t torch.Size([256])\n",
      "147 layer3.3.bn2.running_mean \t torch.Size([256])\n",
      "148 layer3.3.bn2.running_var \t torch.Size([256])\n",
      "149 layer3.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "150 layer3.4.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "151 layer3.4.bn1.weight \t torch.Size([256])\n",
      "152 layer3.4.bn1.bias \t torch.Size([256])\n",
      "153 layer3.4.bn1.running_mean \t torch.Size([256])\n",
      "154 layer3.4.bn1.running_var \t torch.Size([256])\n",
      "155 layer3.4.bn1.num_batches_tracked \t torch.Size([])\n",
      "156 layer3.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "157 layer3.4.bn2.weight \t torch.Size([256])\n",
      "158 layer3.4.bn2.bias \t torch.Size([256])\n",
      "159 layer3.4.bn2.running_mean \t torch.Size([256])\n",
      "160 layer3.4.bn2.running_var \t torch.Size([256])\n",
      "161 layer3.4.bn2.num_batches_tracked \t torch.Size([])\n",
      "162 layer3.5.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "163 layer3.5.bn1.weight \t torch.Size([256])\n",
      "164 layer3.5.bn1.bias \t torch.Size([256])\n",
      "165 layer3.5.bn1.running_mean \t torch.Size([256])\n",
      "166 layer3.5.bn1.running_var \t torch.Size([256])\n",
      "167 layer3.5.bn1.num_batches_tracked \t torch.Size([])\n",
      "168 layer3.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "169 layer3.5.bn2.weight \t torch.Size([256])\n",
      "170 layer3.5.bn2.bias \t torch.Size([256])\n",
      "171 layer3.5.bn2.running_mean \t torch.Size([256])\n",
      "172 layer3.5.bn2.running_var \t torch.Size([256])\n",
      "173 layer3.5.bn2.num_batches_tracked \t torch.Size([])\n",
      "174 layer4.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "175 layer4.0.bn1.weight \t torch.Size([512])\n",
      "176 layer4.0.bn1.bias \t torch.Size([512])\n",
      "177 layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "178 layer4.0.bn1.running_var \t torch.Size([512])\n",
      "179 layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "180 layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "181 layer4.0.bn2.weight \t torch.Size([512])\n",
      "182 layer4.0.bn2.bias \t torch.Size([512])\n",
      "183 layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "184 layer4.0.bn2.running_var \t torch.Size([512])\n",
      "185 layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "186 layer4.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "187 layer4.0.downsample.1.weight \t torch.Size([512])\n",
      "188 layer4.0.downsample.1.bias \t torch.Size([512])\n",
      "189 layer4.0.downsample.1.running_mean \t torch.Size([512])\n",
      "190 layer4.0.downsample.1.running_var \t torch.Size([512])\n",
      "191 layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "192 layer4.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "193 layer4.1.bn1.weight \t torch.Size([512])\n",
      "194 layer4.1.bn1.bias \t torch.Size([512])\n",
      "195 layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "196 layer4.1.bn1.running_var \t torch.Size([512])\n",
      "197 layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "198 layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "199 layer4.1.bn2.weight \t torch.Size([512])\n",
      "200 layer4.1.bn2.bias \t torch.Size([512])\n",
      "201 layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "202 layer4.1.bn2.running_var \t torch.Size([512])\n",
      "203 layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "204 layer4.2.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "205 layer4.2.bn1.weight \t torch.Size([512])\n",
      "206 layer4.2.bn1.bias \t torch.Size([512])\n",
      "207 layer4.2.bn1.running_mean \t torch.Size([512])\n",
      "208 layer4.2.bn1.running_var \t torch.Size([512])\n",
      "209 layer4.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "210 layer4.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "211 layer4.2.bn2.weight \t torch.Size([512])\n",
      "212 layer4.2.bn2.bias \t torch.Size([512])\n",
      "213 layer4.2.bn2.running_mean \t torch.Size([512])\n",
      "214 layer4.2.bn2.running_var \t torch.Size([512])\n",
      "215 layer4.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "216 fc_action.weight \t torch.Size([2, 512])\n",
      "217 fc_action.bias \t torch.Size([2])\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "filter out unnecessary keys pretrained_dict's state_dict: 180\n",
      "0 conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "1 bn1.running_mean \t torch.Size([64])\n",
      "2 bn1.running_var \t torch.Size([64])\n",
      "3 bn1.weight \t torch.Size([64])\n",
      "4 bn1.bias \t torch.Size([64])\n",
      "5 layer1.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "6 layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "7 layer1.0.bn1.running_var \t torch.Size([64])\n",
      "8 layer1.0.bn1.weight \t torch.Size([64])\n",
      "9 layer1.0.bn1.bias \t torch.Size([64])\n",
      "10 layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "11 layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "12 layer1.0.bn2.running_var \t torch.Size([64])\n",
      "13 layer1.0.bn2.weight \t torch.Size([64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 layer1.0.bn2.bias \t torch.Size([64])\n",
      "15 layer1.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "16 layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "17 layer1.1.bn1.running_var \t torch.Size([64])\n",
      "18 layer1.1.bn1.weight \t torch.Size([64])\n",
      "19 layer1.1.bn1.bias \t torch.Size([64])\n",
      "20 layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "21 layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "22 layer1.1.bn2.running_var \t torch.Size([64])\n",
      "23 layer1.1.bn2.weight \t torch.Size([64])\n",
      "24 layer1.1.bn2.bias \t torch.Size([64])\n",
      "25 layer1.2.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "26 layer1.2.bn1.running_mean \t torch.Size([64])\n",
      "27 layer1.2.bn1.running_var \t torch.Size([64])\n",
      "28 layer1.2.bn1.weight \t torch.Size([64])\n",
      "29 layer1.2.bn1.bias \t torch.Size([64])\n",
      "30 layer1.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "31 layer1.2.bn2.running_mean \t torch.Size([64])\n",
      "32 layer1.2.bn2.running_var \t torch.Size([64])\n",
      "33 layer1.2.bn2.weight \t torch.Size([64])\n",
      "34 layer1.2.bn2.bias \t torch.Size([64])\n",
      "35 layer2.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "36 layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "37 layer2.0.bn1.running_var \t torch.Size([128])\n",
      "38 layer2.0.bn1.weight \t torch.Size([128])\n",
      "39 layer2.0.bn1.bias \t torch.Size([128])\n",
      "40 layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "41 layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "42 layer2.0.bn2.running_var \t torch.Size([128])\n",
      "43 layer2.0.bn2.weight \t torch.Size([128])\n",
      "44 layer2.0.bn2.bias \t torch.Size([128])\n",
      "45 layer2.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "46 layer2.0.downsample.1.running_mean \t torch.Size([128])\n",
      "47 layer2.0.downsample.1.running_var \t torch.Size([128])\n",
      "48 layer2.0.downsample.1.weight \t torch.Size([128])\n",
      "49 layer2.0.downsample.1.bias \t torch.Size([128])\n",
      "50 layer2.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "51 layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "52 layer2.1.bn1.running_var \t torch.Size([128])\n",
      "53 layer2.1.bn1.weight \t torch.Size([128])\n",
      "54 layer2.1.bn1.bias \t torch.Size([128])\n",
      "55 layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "56 layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "57 layer2.1.bn2.running_var \t torch.Size([128])\n",
      "58 layer2.1.bn2.weight \t torch.Size([128])\n",
      "59 layer2.1.bn2.bias \t torch.Size([128])\n",
      "60 layer2.2.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "61 layer2.2.bn1.running_mean \t torch.Size([128])\n",
      "62 layer2.2.bn1.running_var \t torch.Size([128])\n",
      "63 layer2.2.bn1.weight \t torch.Size([128])\n",
      "64 layer2.2.bn1.bias \t torch.Size([128])\n",
      "65 layer2.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "66 layer2.2.bn2.running_mean \t torch.Size([128])\n",
      "67 layer2.2.bn2.running_var \t torch.Size([128])\n",
      "68 layer2.2.bn2.weight \t torch.Size([128])\n",
      "69 layer2.2.bn2.bias \t torch.Size([128])\n",
      "70 layer2.3.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "71 layer2.3.bn1.running_mean \t torch.Size([128])\n",
      "72 layer2.3.bn1.running_var \t torch.Size([128])\n",
      "73 layer2.3.bn1.weight \t torch.Size([128])\n",
      "74 layer2.3.bn1.bias \t torch.Size([128])\n",
      "75 layer2.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "76 layer2.3.bn2.running_mean \t torch.Size([128])\n",
      "77 layer2.3.bn2.running_var \t torch.Size([128])\n",
      "78 layer2.3.bn2.weight \t torch.Size([128])\n",
      "79 layer2.3.bn2.bias \t torch.Size([128])\n",
      "80 layer3.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "81 layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "82 layer3.0.bn1.running_var \t torch.Size([256])\n",
      "83 layer3.0.bn1.weight \t torch.Size([256])\n",
      "84 layer3.0.bn1.bias \t torch.Size([256])\n",
      "85 layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "86 layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "87 layer3.0.bn2.running_var \t torch.Size([256])\n",
      "88 layer3.0.bn2.weight \t torch.Size([256])\n",
      "89 layer3.0.bn2.bias \t torch.Size([256])\n",
      "90 layer3.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "91 layer3.0.downsample.1.running_mean \t torch.Size([256])\n",
      "92 layer3.0.downsample.1.running_var \t torch.Size([256])\n",
      "93 layer3.0.downsample.1.weight \t torch.Size([256])\n",
      "94 layer3.0.downsample.1.bias \t torch.Size([256])\n",
      "95 layer3.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "96 layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "97 layer3.1.bn1.running_var \t torch.Size([256])\n",
      "98 layer3.1.bn1.weight \t torch.Size([256])\n",
      "99 layer3.1.bn1.bias \t torch.Size([256])\n",
      "100 layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "101 layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "102 layer3.1.bn2.running_var \t torch.Size([256])\n",
      "103 layer3.1.bn2.weight \t torch.Size([256])\n",
      "104 layer3.1.bn2.bias \t torch.Size([256])\n",
      "105 layer3.2.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "106 layer3.2.bn1.running_mean \t torch.Size([256])\n",
      "107 layer3.2.bn1.running_var \t torch.Size([256])\n",
      "108 layer3.2.bn1.weight \t torch.Size([256])\n",
      "109 layer3.2.bn1.bias \t torch.Size([256])\n",
      "110 layer3.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "111 layer3.2.bn2.running_mean \t torch.Size([256])\n",
      "112 layer3.2.bn2.running_var \t torch.Size([256])\n",
      "113 layer3.2.bn2.weight \t torch.Size([256])\n",
      "114 layer3.2.bn2.bias \t torch.Size([256])\n",
      "115 layer3.3.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "116 layer3.3.bn1.running_mean \t torch.Size([256])\n",
      "117 layer3.3.bn1.running_var \t torch.Size([256])\n",
      "118 layer3.3.bn1.weight \t torch.Size([256])\n",
      "119 layer3.3.bn1.bias \t torch.Size([256])\n",
      "120 layer3.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "121 layer3.3.bn2.running_mean \t torch.Size([256])\n",
      "122 layer3.3.bn2.running_var \t torch.Size([256])\n",
      "123 layer3.3.bn2.weight \t torch.Size([256])\n",
      "124 layer3.3.bn2.bias \t torch.Size([256])\n",
      "125 layer3.4.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "126 layer3.4.bn1.running_mean \t torch.Size([256])\n",
      "127 layer3.4.bn1.running_var \t torch.Size([256])\n",
      "128 layer3.4.bn1.weight \t torch.Size([256])\n",
      "129 layer3.4.bn1.bias \t torch.Size([256])\n",
      "130 layer3.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "131 layer3.4.bn2.running_mean \t torch.Size([256])\n",
      "132 layer3.4.bn2.running_var \t torch.Size([256])\n",
      "133 layer3.4.bn2.weight \t torch.Size([256])\n",
      "134 layer3.4.bn2.bias \t torch.Size([256])\n",
      "135 layer3.5.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "136 layer3.5.bn1.running_mean \t torch.Size([256])\n",
      "137 layer3.5.bn1.running_var \t torch.Size([256])\n",
      "138 layer3.5.bn1.weight \t torch.Size([256])\n",
      "139 layer3.5.bn1.bias \t torch.Size([256])\n",
      "140 layer3.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "141 layer3.5.bn2.running_mean \t torch.Size([256])\n",
      "142 layer3.5.bn2.running_var \t torch.Size([256])\n",
      "143 layer3.5.bn2.weight \t torch.Size([256])\n",
      "144 layer3.5.bn2.bias \t torch.Size([256])\n",
      "145 layer4.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "146 layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "147 layer4.0.bn1.running_var \t torch.Size([512])\n",
      "148 layer4.0.bn1.weight \t torch.Size([512])\n",
      "149 layer4.0.bn1.bias \t torch.Size([512])\n",
      "150 layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "151 layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "152 layer4.0.bn2.running_var \t torch.Size([512])\n",
      "153 layer4.0.bn2.weight \t torch.Size([512])\n",
      "154 layer4.0.bn2.bias \t torch.Size([512])\n",
      "155 layer4.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "156 layer4.0.downsample.1.running_mean \t torch.Size([512])\n",
      "157 layer4.0.downsample.1.running_var \t torch.Size([512])\n",
      "158 layer4.0.downsample.1.weight \t torch.Size([512])\n",
      "159 layer4.0.downsample.1.bias \t torch.Size([512])\n",
      "160 layer4.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "161 layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "162 layer4.1.bn1.running_var \t torch.Size([512])\n",
      "163 layer4.1.bn1.weight \t torch.Size([512])\n",
      "164 layer4.1.bn1.bias \t torch.Size([512])\n",
      "165 layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "166 layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "167 layer4.1.bn2.running_var \t torch.Size([512])\n",
      "168 layer4.1.bn2.weight \t torch.Size([512])\n",
      "169 layer4.1.bn2.bias \t torch.Size([512])\n",
      "170 layer4.2.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "171 layer4.2.bn1.running_mean \t torch.Size([512])\n",
      "172 layer4.2.bn1.running_var \t torch.Size([512])\n",
      "173 layer4.2.bn1.weight \t torch.Size([512])\n",
      "174 layer4.2.bn1.bias \t torch.Size([512])\n",
      "175 layer4.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "176 layer4.2.bn2.running_mean \t torch.Size([512])\n",
      "177 layer4.2.bn2.running_var \t torch.Size([512])\n",
      "178 layer4.2.bn2.weight \t torch.Size([512])\n",
      "179 layer4.2.bn2.bias \t torch.Size([512])\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "update model_dict's state_dict: 218\n",
      "0 conv1.weight \t torch.Size([64, 3, 7, 7])\n",
      "1 bn1.weight \t torch.Size([64])\n",
      "2 bn1.bias \t torch.Size([64])\n",
      "3 bn1.running_mean \t torch.Size([64])\n",
      "4 bn1.running_var \t torch.Size([64])\n",
      "5 bn1.num_batches_tracked \t torch.Size([])\n",
      "6 layer1.0.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "7 layer1.0.bn1.weight \t torch.Size([64])\n",
      "8 layer1.0.bn1.bias \t torch.Size([64])\n",
      "9 layer1.0.bn1.running_mean \t torch.Size([64])\n",
      "10 layer1.0.bn1.running_var \t torch.Size([64])\n",
      "11 layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "12 layer1.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "13 layer1.0.bn2.weight \t torch.Size([64])\n",
      "14 layer1.0.bn2.bias \t torch.Size([64])\n",
      "15 layer1.0.bn2.running_mean \t torch.Size([64])\n",
      "16 layer1.0.bn2.running_var \t torch.Size([64])\n",
      "17 layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "18 layer1.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "19 layer1.1.bn1.weight \t torch.Size([64])\n",
      "20 layer1.1.bn1.bias \t torch.Size([64])\n",
      "21 layer1.1.bn1.running_mean \t torch.Size([64])\n",
      "22 layer1.1.bn1.running_var \t torch.Size([64])\n",
      "23 layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "24 layer1.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "25 layer1.1.bn2.weight \t torch.Size([64])\n",
      "26 layer1.1.bn2.bias \t torch.Size([64])\n",
      "27 layer1.1.bn2.running_mean \t torch.Size([64])\n",
      "28 layer1.1.bn2.running_var \t torch.Size([64])\n",
      "29 layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "30 layer1.2.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
      "31 layer1.2.bn1.weight \t torch.Size([64])\n",
      "32 layer1.2.bn1.bias \t torch.Size([64])\n",
      "33 layer1.2.bn1.running_mean \t torch.Size([64])\n",
      "34 layer1.2.bn1.running_var \t torch.Size([64])\n",
      "35 layer1.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "36 layer1.2.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
      "37 layer1.2.bn2.weight \t torch.Size([64])\n",
      "38 layer1.2.bn2.bias \t torch.Size([64])\n",
      "39 layer1.2.bn2.running_mean \t torch.Size([64])\n",
      "40 layer1.2.bn2.running_var \t torch.Size([64])\n",
      "41 layer1.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "42 layer2.0.conv1.weight \t torch.Size([128, 64, 3, 3])\n",
      "43 layer2.0.bn1.weight \t torch.Size([128])\n",
      "44 layer2.0.bn1.bias \t torch.Size([128])\n",
      "45 layer2.0.bn1.running_mean \t torch.Size([128])\n",
      "46 layer2.0.bn1.running_var \t torch.Size([128])\n",
      "47 layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "48 layer2.0.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "49 layer2.0.bn2.weight \t torch.Size([128])\n",
      "50 layer2.0.bn2.bias \t torch.Size([128])\n",
      "51 layer2.0.bn2.running_mean \t torch.Size([128])\n",
      "52 layer2.0.bn2.running_var \t torch.Size([128])\n",
      "53 layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "54 layer2.0.downsample.0.weight \t torch.Size([128, 64, 1, 1])\n",
      "55 layer2.0.downsample.1.weight \t torch.Size([128])\n",
      "56 layer2.0.downsample.1.bias \t torch.Size([128])\n",
      "57 layer2.0.downsample.1.running_mean \t torch.Size([128])\n",
      "58 layer2.0.downsample.1.running_var \t torch.Size([128])\n",
      "59 layer2.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "60 layer2.1.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "61 layer2.1.bn1.weight \t torch.Size([128])\n",
      "62 layer2.1.bn1.bias \t torch.Size([128])\n",
      "63 layer2.1.bn1.running_mean \t torch.Size([128])\n",
      "64 layer2.1.bn1.running_var \t torch.Size([128])\n",
      "65 layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "66 layer2.1.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "67 layer2.1.bn2.weight \t torch.Size([128])\n",
      "68 layer2.1.bn2.bias \t torch.Size([128])\n",
      "69 layer2.1.bn2.running_mean \t torch.Size([128])\n",
      "70 layer2.1.bn2.running_var \t torch.Size([128])\n",
      "71 layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "72 layer2.2.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "73 layer2.2.bn1.weight \t torch.Size([128])\n",
      "74 layer2.2.bn1.bias \t torch.Size([128])\n",
      "75 layer2.2.bn1.running_mean \t torch.Size([128])\n",
      "76 layer2.2.bn1.running_var \t torch.Size([128])\n",
      "77 layer2.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "78 layer2.2.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "79 layer2.2.bn2.weight \t torch.Size([128])\n",
      "80 layer2.2.bn2.bias \t torch.Size([128])\n",
      "81 layer2.2.bn2.running_mean \t torch.Size([128])\n",
      "82 layer2.2.bn2.running_var \t torch.Size([128])\n",
      "83 layer2.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "84 layer2.3.conv1.weight \t torch.Size([128, 128, 3, 3])\n",
      "85 layer2.3.bn1.weight \t torch.Size([128])\n",
      "86 layer2.3.bn1.bias \t torch.Size([128])\n",
      "87 layer2.3.bn1.running_mean \t torch.Size([128])\n",
      "88 layer2.3.bn1.running_var \t torch.Size([128])\n",
      "89 layer2.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "90 layer2.3.conv2.weight \t torch.Size([128, 128, 3, 3])\n",
      "91 layer2.3.bn2.weight \t torch.Size([128])\n",
      "92 layer2.3.bn2.bias \t torch.Size([128])\n",
      "93 layer2.3.bn2.running_mean \t torch.Size([128])\n",
      "94 layer2.3.bn2.running_var \t torch.Size([128])\n",
      "95 layer2.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "96 layer3.0.conv1.weight \t torch.Size([256, 128, 3, 3])\n",
      "97 layer3.0.bn1.weight \t torch.Size([256])\n",
      "98 layer3.0.bn1.bias \t torch.Size([256])\n",
      "99 layer3.0.bn1.running_mean \t torch.Size([256])\n",
      "100 layer3.0.bn1.running_var \t torch.Size([256])\n",
      "101 layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "102 layer3.0.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "103 layer3.0.bn2.weight \t torch.Size([256])\n",
      "104 layer3.0.bn2.bias \t torch.Size([256])\n",
      "105 layer3.0.bn2.running_mean \t torch.Size([256])\n",
      "106 layer3.0.bn2.running_var \t torch.Size([256])\n",
      "107 layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "108 layer3.0.downsample.0.weight \t torch.Size([256, 128, 1, 1])\n",
      "109 layer3.0.downsample.1.weight \t torch.Size([256])\n",
      "110 layer3.0.downsample.1.bias \t torch.Size([256])\n",
      "111 layer3.0.downsample.1.running_mean \t torch.Size([256])\n",
      "112 layer3.0.downsample.1.running_var \t torch.Size([256])\n",
      "113 layer3.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "114 layer3.1.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "115 layer3.1.bn1.weight \t torch.Size([256])\n",
      "116 layer3.1.bn1.bias \t torch.Size([256])\n",
      "117 layer3.1.bn1.running_mean \t torch.Size([256])\n",
      "118 layer3.1.bn1.running_var \t torch.Size([256])\n",
      "119 layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "120 layer3.1.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "121 layer3.1.bn2.weight \t torch.Size([256])\n",
      "122 layer3.1.bn2.bias \t torch.Size([256])\n",
      "123 layer3.1.bn2.running_mean \t torch.Size([256])\n",
      "124 layer3.1.bn2.running_var \t torch.Size([256])\n",
      "125 layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "126 layer3.2.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "127 layer3.2.bn1.weight \t torch.Size([256])\n",
      "128 layer3.2.bn1.bias \t torch.Size([256])\n",
      "129 layer3.2.bn1.running_mean \t torch.Size([256])\n",
      "130 layer3.2.bn1.running_var \t torch.Size([256])\n",
      "131 layer3.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "132 layer3.2.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "133 layer3.2.bn2.weight \t torch.Size([256])\n",
      "134 layer3.2.bn2.bias \t torch.Size([256])\n",
      "135 layer3.2.bn2.running_mean \t torch.Size([256])\n",
      "136 layer3.2.bn2.running_var \t torch.Size([256])\n",
      "137 layer3.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "138 layer3.3.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "139 layer3.3.bn1.weight \t torch.Size([256])\n",
      "140 layer3.3.bn1.bias \t torch.Size([256])\n",
      "141 layer3.3.bn1.running_mean \t torch.Size([256])\n",
      "142 layer3.3.bn1.running_var \t torch.Size([256])\n",
      "143 layer3.3.bn1.num_batches_tracked \t torch.Size([])\n",
      "144 layer3.3.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "145 layer3.3.bn2.weight \t torch.Size([256])\n",
      "146 layer3.3.bn2.bias \t torch.Size([256])\n",
      "147 layer3.3.bn2.running_mean \t torch.Size([256])\n",
      "148 layer3.3.bn2.running_var \t torch.Size([256])\n",
      "149 layer3.3.bn2.num_batches_tracked \t torch.Size([])\n",
      "150 layer3.4.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "151 layer3.4.bn1.weight \t torch.Size([256])\n",
      "152 layer3.4.bn1.bias \t torch.Size([256])\n",
      "153 layer3.4.bn1.running_mean \t torch.Size([256])\n",
      "154 layer3.4.bn1.running_var \t torch.Size([256])\n",
      "155 layer3.4.bn1.num_batches_tracked \t torch.Size([])\n",
      "156 layer3.4.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "157 layer3.4.bn2.weight \t torch.Size([256])\n",
      "158 layer3.4.bn2.bias \t torch.Size([256])\n",
      "159 layer3.4.bn2.running_mean \t torch.Size([256])\n",
      "160 layer3.4.bn2.running_var \t torch.Size([256])\n",
      "161 layer3.4.bn2.num_batches_tracked \t torch.Size([])\n",
      "162 layer3.5.conv1.weight \t torch.Size([256, 256, 3, 3])\n",
      "163 layer3.5.bn1.weight \t torch.Size([256])\n",
      "164 layer3.5.bn1.bias \t torch.Size([256])\n",
      "165 layer3.5.bn1.running_mean \t torch.Size([256])\n",
      "166 layer3.5.bn1.running_var \t torch.Size([256])\n",
      "167 layer3.5.bn1.num_batches_tracked \t torch.Size([])\n",
      "168 layer3.5.conv2.weight \t torch.Size([256, 256, 3, 3])\n",
      "169 layer3.5.bn2.weight \t torch.Size([256])\n",
      "170 layer3.5.bn2.bias \t torch.Size([256])\n",
      "171 layer3.5.bn2.running_mean \t torch.Size([256])\n",
      "172 layer3.5.bn2.running_var \t torch.Size([256])\n",
      "173 layer3.5.bn2.num_batches_tracked \t torch.Size([])\n",
      "174 layer4.0.conv1.weight \t torch.Size([512, 256, 3, 3])\n",
      "175 layer4.0.bn1.weight \t torch.Size([512])\n",
      "176 layer4.0.bn1.bias \t torch.Size([512])\n",
      "177 layer4.0.bn1.running_mean \t torch.Size([512])\n",
      "178 layer4.0.bn1.running_var \t torch.Size([512])\n",
      "179 layer4.0.bn1.num_batches_tracked \t torch.Size([])\n",
      "180 layer4.0.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "181 layer4.0.bn2.weight \t torch.Size([512])\n",
      "182 layer4.0.bn2.bias \t torch.Size([512])\n",
      "183 layer4.0.bn2.running_mean \t torch.Size([512])\n",
      "184 layer4.0.bn2.running_var \t torch.Size([512])\n",
      "185 layer4.0.bn2.num_batches_tracked \t torch.Size([])\n",
      "186 layer4.0.downsample.0.weight \t torch.Size([512, 256, 1, 1])\n",
      "187 layer4.0.downsample.1.weight \t torch.Size([512])\n",
      "188 layer4.0.downsample.1.bias \t torch.Size([512])\n",
      "189 layer4.0.downsample.1.running_mean \t torch.Size([512])\n",
      "190 layer4.0.downsample.1.running_var \t torch.Size([512])\n",
      "191 layer4.0.downsample.1.num_batches_tracked \t torch.Size([])\n",
      "192 layer4.1.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "193 layer4.1.bn1.weight \t torch.Size([512])\n",
      "194 layer4.1.bn1.bias \t torch.Size([512])\n",
      "195 layer4.1.bn1.running_mean \t torch.Size([512])\n",
      "196 layer4.1.bn1.running_var \t torch.Size([512])\n",
      "197 layer4.1.bn1.num_batches_tracked \t torch.Size([])\n",
      "198 layer4.1.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "199 layer4.1.bn2.weight \t torch.Size([512])\n",
      "200 layer4.1.bn2.bias \t torch.Size([512])\n",
      "201 layer4.1.bn2.running_mean \t torch.Size([512])\n",
      "202 layer4.1.bn2.running_var \t torch.Size([512])\n",
      "203 layer4.1.bn2.num_batches_tracked \t torch.Size([])\n",
      "204 layer4.2.conv1.weight \t torch.Size([512, 512, 3, 3])\n",
      "205 layer4.2.bn1.weight \t torch.Size([512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206 layer4.2.bn1.bias \t torch.Size([512])\n",
      "207 layer4.2.bn1.running_mean \t torch.Size([512])\n",
      "208 layer4.2.bn1.running_var \t torch.Size([512])\n",
      "209 layer4.2.bn1.num_batches_tracked \t torch.Size([])\n",
      "210 layer4.2.conv2.weight \t torch.Size([512, 512, 3, 3])\n",
      "211 layer4.2.bn2.weight \t torch.Size([512])\n",
      "212 layer4.2.bn2.bias \t torch.Size([512])\n",
      "213 layer4.2.bn2.running_mean \t torch.Size([512])\n",
      "214 layer4.2.bn2.running_var \t torch.Size([512])\n",
      "215 layer4.2.bn2.num_batches_tracked \t torch.Size([])\n",
      "216 fc_action.weight \t torch.Size([2, 512])\n",
      "217 fc_action.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "model = rgb_resnet34(True, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bill': 21, 'rich': 10, 'aa': 20}\n"
     ]
    }
   ],
   "source": [
    "my_dict = {'bill':3, 'rich':10}\n",
    "yourDict = {'bill':21, 'aa':20}\n",
    "my_dict.update(yourDict)\n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('a', 'A'), ('b', 'B'), ('c', 'C'), ('c1', 'C'), ('a1', 'A'), ('b1', 'B')])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "d4=collections.OrderedDict()\n",
    "d4['a']='A'\n",
    "d4['b']='B'\n",
    "d4['c']='C'\n",
    "\n",
    "d5=collections.OrderedDict()\n",
    "d5['c1']='C'\n",
    "d5['a1']='A'\n",
    "d5['b1']='B'\n",
    "d4.update(d5)\n",
    "print(d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['a', 'b', 'c', 'c1', 'a1', 'b1'])\n"
     ]
    }
   ],
   "source": [
    "allKeyList = d4.keys()\n",
    "print(allKeyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'c1', 'a1', 'b1']\n"
     ]
    }
   ],
   "source": [
    "lst = list(allKeyList)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['layer1', '0.conv1.weight'] layer1_a.0.conv1.weight\n"
     ]
    }
   ],
   "source": [
    "layer_key = 'layer1.0.conv1.weight'\n",
    "lay= layer_key.split('.', 1)\n",
    "new_layer_name = '_a.'.join(lay)\n",
    "print(lay, new_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
